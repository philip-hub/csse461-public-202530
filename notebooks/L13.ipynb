{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2849caae-6882-48be-9e5d-53b2aefd4692",
   "metadata": {},
   "source": [
    "# Lecture 13 - Underfitting/Overfitting, and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424afae-03bf-49c6-a09a-29556939bf0a",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "After this class, students will be able to\n",
    "- explain what under- and over-fitting are, and the need for train/test splits\n",
    "- identify the common elements of ML models for Vision:\n",
    "  - Neural Layers: convolutional, fully connected\n",
    "  - Parameter-less Layers: pooling, batch norm, downsample, upsample, flatten\n",
    "  - Activations: sigmoid, ReLU, nod at others\n",
    "  - Losses: cross entropy, L1, L2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc56a3-6511-48f0-9a11-92c8911a39db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm running today's lecture on a GPU server\n",
    "# This line chooses which GPU I'll be running on. Run this line before any other imports. \n",
    "# To change GPUs, restart your kernel and then rerun this cell.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Use this GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33aa1a-80bd-47f2-9987-899278247a36",
   "metadata": {},
   "source": [
    "## Theory topic: Underfitting and Overfitting\n",
    "\n",
    "Underfitting and overfitting are universally useful concepts in ML, but it's hard to directly visualize these in the context of neural networks. Neural nets are large, complex, and high dimensional. So we're going to play with a toy problem that's easy to visualize.\n",
    "\n",
    "Our demo problem:\n",
    "- Input: a single real number\n",
    "- Output: a single real number\n",
    "- Fake data: a simple cubic function + Gaussian noise\n",
    "- Models: polynomials of degree k (we'll try various values of k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e4912-492b-40f2-8647-7227d4a6b586",
   "metadata": {},
   "source": [
    "#### Problem setup: random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368a715-b997-4b01-a1d7-6d2d31fcb969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the under/over-fitting demo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf6ea7b-a038-4a0d-851d-0c8d977e593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed a random number generator.\n",
    "# This is overkill, but I finally got around to looking \n",
    "# up the official \"right\" way to do it.\n",
    "from numpy.random import MT19937\n",
    "from numpy.random import RandomState, SeedSequence\n",
    "rs = RandomState(MT19937(SeedSequence(123)))\n",
    "\n",
    "\n",
    "###################\n",
    "# data parameters #\n",
    "###################\n",
    "# (are fancy comment boxes like this tacky?)\n",
    "n_train = 30\n",
    "n_test = 50\n",
    "noise_sd = 0.008\n",
    "\n",
    "# The numpy.polynomial package has a really convenient function \n",
    "# for constructing polynomials from a list of roots.\n",
    "# But it uses the opposite storage convention from polyfit and polyval.\n",
    "true_model = np.polynomial.polynomial.polyfromroots([0.15, 0.35, .45, 0.9])\n",
    "true_model = true_model[::-1] # reverse the list\n",
    "\n",
    "# make the x coords of the data\n",
    "x_train = np.sort(rs.uniform(0, 1, n_train))\n",
    "x_test = np.sort(rs.uniform(0, 1, n_test))\n",
    "\n",
    "# make the y coords of the data\n",
    "y_train = np.polyval(true_model, x_train) + rs.normal(0, noise_sd, n_train)\n",
    "y_test = np.polyval(true_model, x_test) + rs.normal(0, noise_sd, n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40477de-5ef8-4303-9fc0-116bdb4f0627",
   "metadata": {},
   "source": [
    "#### Check our work:\n",
    "plot the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618b73eb-0ac4-4013-8549-d23f39c670dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model and the data\n",
    "plt.plot(x_train, y_train, '.b', label='train set')\n",
    "plt.plot(x_test, y_test, '.r', label='test set')\n",
    "\n",
    "x_model = np.linspace(0, 1, 200)\n",
    "y_model = np.polyval(true_model, x_model)\n",
    "plt.plot(x_model, y_model, '-k', label='true function')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e98f2f-4a1c-4d26-94ae-0310a6389d24",
   "metadata": {},
   "source": [
    "#### Experiment: which model fits best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019f492-5327-49f9-abef-2150d03e8403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(degree, ax):\n",
    "    \n",
    "    model = np.polyfit(x_train, y_train, degree)\n",
    "    \n",
    "    # Plot the model and the data\n",
    "    ax.plot(x_train, y_train, '.b', label='train set')\n",
    "    #ax.plot(x_test, y_test, '.r', label='test set')\n",
    "\n",
    "    x_model = np.linspace(0, 1, 200)\n",
    "    y_model = np.polyval(model, x_model)\n",
    "    ax.plot(x_model, y_model, '-k', label=f'degree {degree} model')\n",
    "\n",
    "    train_errors = y_train - np.polyval(model, x_train)\n",
    "    test_errors = y_test - np.polyval(model, x_test)\n",
    "    train_mae = np.average(np.absolute(train_errors))\n",
    "    test_mae = np.average(np.absolute(test_errors))\n",
    "\n",
    "    perf_text = 'train mae: {:.2e}\\ntest mae: {:.2e}'.format(train_mae, test_mae)\n",
    "    ax.text(0.5, 0.95, perf_text,\n",
    "            verticalalignment='top', horizontalalignment='center',\n",
    "            transform=ax.transAxes,\n",
    "            color='black', fontsize=10)\n",
    "\n",
    "    deg_text = f'degree {degree} model'\n",
    "    ax.text(0.02, 0.02, deg_text,\n",
    "            verticalalignment='bottom', horizontalalignment='left',\n",
    "            transform=ax.transAxes,\n",
    "            color='black', fontsize=10)\n",
    "\n",
    "    ax.set_ylim([-0.05, 0.05])\n",
    "    ax.get_xaxis().set_ticks([])\n",
    "    ax.get_yaxis().set_ticks([])\n",
    "    #ax.set_xlabel('x')\n",
    "    #ax.set_ylabel('y')\n",
    "    # ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79a963-a2f2-4316-85a0-9cf4688fedbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 5\n",
    "ncols = 4\n",
    "nplots = ncols * nrows\n",
    "fig, axarr = plt.subplots(nrows, ncols, sharex=True, figsize=(16,12))\n",
    "for degree in range(0, nplots):\n",
    "    experiment(degree, axarr[degree // ncols][degree % ncols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4007fcac-772f-4dd7-8217-85d01f6533e6",
   "metadata": {},
   "source": [
    "#### Experiment: Peformance vs Model Complexity\n",
    "Redo the experiment above. But don't plot the individual models. Just plot the performance of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d8532-2c95-45f9-9541-2d61fe0320f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.arange(0, 12)\n",
    "train_mae = []\n",
    "test_mae = []\n",
    "\n",
    "for degree in degrees:\n",
    "    model = np.polyfit(x_train, y_train, degree)\n",
    "    \n",
    "    train_errors = y_train - np.polyval(model, x_train)\n",
    "    test_errors = y_test - np.polyval(model, x_test)\n",
    "    train_mae.append(np.average(np.absolute(train_errors)))\n",
    "    test_mae.append(np.average(np.absolute(test_errors)))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5,8))\n",
    "ax.plot(degrees, train_mae, '-b', label='Train set')\n",
    "ax.plot(degrees, test_mae, '-r', label='Test set')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.grid('on')\n",
    "ax.set_xlabel('Model complexity (degree)')\n",
    "ax.set_ylabel('Performance (MAE - mean absolute error)')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf8a04-f8ca-45bc-b3ee-62de907bbb51",
   "metadata": {},
   "source": [
    "### Summary: Model Complexity\n",
    "\n",
    "Bigger models have more parameters. They can fit more things than simple models. Training performance pretty much always improves with higher complexity, but that isn't real! When we check against the test set we see that there's a sweet spot for the right complexity, and anything more than that makes us do worse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a61ca-4cba-40ce-8d1a-9976b5419789",
   "metadata": {},
   "source": [
    "## Topic: Elements of ML models in computer vision\n",
    "\n",
    "Here's an example neural network from our code last class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb74dc9d-58b4-44b6-9f89-b9df0aff42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c82a2a-2683-4aea-b2e4-c5d823d82ca8",
   "metadata": {},
   "source": [
    "Recall, some layers of models have parameters. Where are the parameters in the model above? How many parameters are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7260cb8e-5175-4872-8814-42a4d04a63f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight     torch.Size([32, 3, 3, 3])\n",
      "conv1.bias       torch.Size([32])\n",
      "conv2.weight     torch.Size([64, 32, 3, 3])\n",
      "conv2.bias       torch.Size([64])\n",
      "fc1.weight       torch.Size([512, 4096])\n",
      "fc1.bias         torch.Size([512])\n",
      "fc2.weight       torch.Size([10, 512])\n",
      "fc2.bias         torch.Size([10])\n",
      "Total num parameters: 2122186\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name.ljust(16), param.shape)\n",
    "print('Total num parameters:', sum([x.numel() for x in model.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb92dd1-f7c5-4b68-851e-c7c7b197b79f",
   "metadata": {},
   "source": [
    "### Whiteboard time: NN layers: FC and Conv2d\n",
    "\n",
    "Summary: \n",
    "- A **fully connected** layer performs the operation $y = W x + b$ where $x$ is the input to the layer, $y$ is the output, $W$ are the *weights*, and $b$ are the *biases*. The weights and biases are the parameters to this layer.\n",
    "- A **convolutional** layer performs the operation $y = x \\ast K + b$ where $x$ is the input to the layer, $y$ is the output, $K$ are the *weights/filters*, and $b$ are the *biases*. The weights and biases are the parameters to this layer.\n",
    "\n",
    "Here's a list of all of the \n",
    "[layers in torch.nn](https://pytorch.org/docs/stable/nn.html).\n",
    "These are the building blocks of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de210ce-8190-421f-bb8e-54f64f77a4ac",
   "metadata": {},
   "source": [
    "### Useful layers:\n",
    "- `torch.nn.Linear` [[docs]](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
    "- `torch.nn.Conv2d` [[docs]](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n",
    "- `torch.nn.MaxPool2d` [[docs]](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)\n",
    "- `torch.nn.UpsampleBilinear2d` [[docs]](https://pytorch.org/docs/stable/generated/torch.nn.UpsamplingBilinear2d.html#torch.nn.UpsamplingBilinear2d)\n",
    "\n",
    "Activation functions:\n",
    "- `torch.nn.ReLU` [[docs]](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)\n",
    "- `torch.nn.Sigmoid` [[docs]](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c0e8e-c822-4b9b-80ca-f7dbaa1b1a48",
   "metadata": {},
   "source": [
    "## Topic: Model Architectures\n",
    "\n",
    "There are so many options to make models! Mix and match layers.\n",
    "\n",
    "In practice, ML engineers don't spend a lot of time searching for the best architectures. Instead, we have collected a short list of famous model architectures that have performed surprisingly well for many problems. Then we tweak them (different input sizes, more or fewer parameters) as needed.\n",
    "\n",
    "(Small lie! With the latest and greatest approaches, most ML engineers don't need to train models from scratch. Later we'll talk about the encoder/decoder pattern and how to use pre-trained *foundation* models as the core of new methods.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba9c3ec-521c-4195-99a0-c048b4ca8533",
   "metadata": {},
   "source": [
    "### Examples of architectures that have worked really well\n",
    "\n",
    "- AlexNet [[link]](https://paperswithcode.com/method/alexnet)\n",
    "- GoogleLeNet [[link]](https://paperswithcode.com/method/googlenet)\n",
    "- ResNet [[link]](https://paperswithcode.com/method/resnet)\n",
    "- U-Net [[link]](https://paperswithcode.com/method/u-net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac4e2a6-304a-48b0-b9ab-48fcb2ec4904",
   "metadata": {},
   "source": [
    "### But what do these architectures mean?\n",
    "\n",
    "There are a couple papers that try to visualize what neural networks are learning.\n",
    "\n",
    "- [[AlexNet viz]](https://paperswithcode.com/paper/visualizing-and-understanding-convolutional)\n",
    "- [[GoogleLeNet viz]](https://distill.pub/2017/feature-visualization/)\n",
    "\n",
    "Click through and look at them!\n",
    "\n",
    "Summary:\n",
    "- The earliest layers learn simple filters: color, edge, orientation\n",
    "- The next layers are learning shape and texture\n",
    "- These models are learning heirarchical knowledge: later layers are based on what was learned in earlier layers\n",
    "- Later layers get increasingly abstract. You start to see \"face\" filters and \"wheel\" filters and the like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ea03a-5f6f-48c3-a9ff-917699ec0d52",
   "metadata": {},
   "source": [
    "## Spare Time: Final project topic clinic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc3d3e7-e109-4e0d-b771-f79afdffac52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csse461",
   "language": "python",
   "name": "csse461"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
