{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2849caae-6882-48be-9e5d-53b2aefd4692",
   "metadata": {},
   "source": [
    "# Lecture 12 - Intro to ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424afae-03bf-49c6-a09a-29556939bf0a",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "After this class, students will be able to\n",
    "- identify the key elements of a supervised ML system: data, model, parameters, loss function, training loop\n",
    "- define \"training\" in ML\n",
    "- prompt an LLM to generate pytorch ML code\n",
    "- explain what under- and over-fitting are, and the need for train/test splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9bc56a3-6511-48f0-9a11-92c8911a39db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm running today's lecture on a GPU server\n",
    "# This line chooses which GPU I'll be running on. Run this line before any other imports. \n",
    "# To change GPUs, restart your kernel and then rerun this cell.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Use this GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bccf26-3fa7-4a05-a3c9-ef8958a60cae",
   "metadata": {},
   "source": [
    "### Approach today: zero to ML, a step at a time\n",
    "Many of you have seen ML fundamentals in other classes. You'll know many of the things we're doing today already. We'll begin by designing systems with serious flaws, and we'll fix them as we discover them. Please play along with the story, and don't drop spoilers too soon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053eb7e1-500c-4084-857d-da66a97ccd76",
   "metadata": {},
   "source": [
    "### Approach today: LLMs\n",
    "We're going to try LLM-assisted coding in class today. I have two goals on this:\n",
    "\n",
    "- Demoing how to use AI to assist in developing ML systems\n",
    "- Keeping today's emphasis on ML ideas, rather than on code syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7325255-6168-4365-a364-9a0f17068a7b",
   "metadata": {},
   "source": [
    "## Plan for today:\n",
    "\n",
    "- [ ] Get a dataset - CIFAR10. Explore and visualize.\n",
    "- [ ] Create \"Dataloaders\"\n",
    "- [ ] Prompt an LLM to get a simple model\n",
    "- [ ] Prompt an LLM to learn about our options for loss functions\n",
    "- [ ] Prompt an LLM to get a simple training loop\n",
    "- [ ] Train our model\n",
    "- [ ] Evaluate the performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ea21a6-7ef2-44fd-9f80-3993347eacac",
   "metadata": {},
   "source": [
    "#### Our Dataset: [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "Why?\n",
    "\n",
    "- it's a vision dataset\n",
    "- the vision problem (image classification) is relatively straightforward\n",
    "- it's small enough we can do training runs live in class\n",
    "\n",
    "pyTorch comes with several built-in datasets, ranging from quite small to very large. Let's use pytorch to load this dataset next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00524c64-cbe3-4105-a62e-5d6200551df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frog  truck truck deer \n",
      "Type of images: <class 'torch.Tensor'>\n",
      "Shape of images batch: torch.Size([4, 3, 32, 32])\n",
      "Shape of a single image: torch.Size([3, 32, 32])\n",
      "Data type of images: torch.float32\n",
      "Type of labels: <class 'list'>\n",
      "Shape of labels: 4\n",
      "Data type of labels: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Transformations to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "classes = trainset.classes\n",
    "\n",
    "# Function to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Get a few random training images\n",
    "images, labels = [], []\n",
    "for i in range(4):\n",
    "    image, label = trainset[i]\n",
    "    images.append(image)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert list of tensors to a single tensor\n",
    "images = torch.stack(images)\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# Print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n",
    "\n",
    "# Exploratory data analysis\n",
    "print(f'Type of images: {type(images)}')\n",
    "print(f'Shape of images batch: {images.shape}')\n",
    "print(f'Shape of a single image: {images[0].shape}')\n",
    "print(f'Data type of images: {images.dtype}')\n",
    "\n",
    "print(f'Type of labels: {type(labels)}')\n",
    "print(f'Shape of labels: {len(labels)}')\n",
    "print(f'Data type of labels: {type(labels[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "025236a0-6001-4e4b-8330-145a6c227a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=4096, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=64 * 8 * 8, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolutional layer with ReLU and pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Second convolutional layer with ReLU and pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatten the feature maps\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        # First fully connected layer with ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Second fully connected layer for output\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleCNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ccf80f5-67ed-4547-a831-de3a9f28ef2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/12500], Loss: 2.2243\n",
      "Epoch [1/10], Step [200/12500], Loss: 2.0970\n",
      "Epoch [1/10], Step [300/12500], Loss: 1.9562\n",
      "Epoch [1/10], Step [400/12500], Loss: 1.8711\n",
      "Epoch [1/10], Step [500/12500], Loss: 1.8451\n",
      "Epoch [1/10], Step [600/12500], Loss: 1.8271\n",
      "Epoch [1/10], Step [700/12500], Loss: 1.7742\n",
      "Epoch [1/10], Step [800/12500], Loss: 1.6216\n",
      "Epoch [1/10], Step [900/12500], Loss: 1.7015\n",
      "Epoch [1/10], Step [1000/12500], Loss: 1.6142\n",
      "Epoch [1/10], Step [1100/12500], Loss: 1.6565\n",
      "Epoch [1/10], Step [1200/12500], Loss: 1.5276\n",
      "Epoch [1/10], Step [1300/12500], Loss: 1.5768\n",
      "Epoch [1/10], Step [1400/12500], Loss: 1.5898\n",
      "Epoch [1/10], Step [1500/12500], Loss: 1.4917\n",
      "Epoch [1/10], Step [1600/12500], Loss: 1.5352\n",
      "Epoch [1/10], Step [1700/12500], Loss: 1.4259\n",
      "Epoch [1/10], Step [1800/12500], Loss: 1.3659\n",
      "Epoch [1/10], Step [1900/12500], Loss: 1.5818\n",
      "Epoch [1/10], Step [2000/12500], Loss: 1.2877\n",
      "Epoch [1/10], Step [2100/12500], Loss: 1.4920\n",
      "Epoch [1/10], Step [2200/12500], Loss: 1.4275\n",
      "Epoch [1/10], Step [2300/12500], Loss: 1.4070\n",
      "Epoch [1/10], Step [2400/12500], Loss: 1.3008\n",
      "Epoch [1/10], Step [2500/12500], Loss: 1.5608\n",
      "Epoch [1/10], Step [2600/12500], Loss: 1.4323\n",
      "Epoch [1/10], Step [2700/12500], Loss: 1.3641\n",
      "Epoch [1/10], Step [2800/12500], Loss: 1.4032\n",
      "Epoch [1/10], Step [2900/12500], Loss: 1.3353\n",
      "Epoch [1/10], Step [3000/12500], Loss: 1.3399\n",
      "Epoch [1/10], Step [3100/12500], Loss: 1.2829\n",
      "Epoch [1/10], Step [3200/12500], Loss: 1.3299\n",
      "Epoch [1/10], Step [3300/12500], Loss: 1.3444\n",
      "Epoch [1/10], Step [3400/12500], Loss: 1.2820\n",
      "Epoch [1/10], Step [3500/12500], Loss: 1.4034\n",
      "Epoch [1/10], Step [3600/12500], Loss: 1.3322\n",
      "Epoch [1/10], Step [3700/12500], Loss: 1.3537\n",
      "Epoch [1/10], Step [3800/12500], Loss: 1.3390\n",
      "Epoch [1/10], Step [3900/12500], Loss: 1.2823\n",
      "Epoch [1/10], Step [4000/12500], Loss: 1.2383\n",
      "Epoch [1/10], Step [4100/12500], Loss: 1.3055\n",
      "Epoch [1/10], Step [4200/12500], Loss: 1.2545\n",
      "Epoch [1/10], Step [4300/12500], Loss: 1.2725\n",
      "Epoch [1/10], Step [4400/12500], Loss: 1.2898\n",
      "Epoch [1/10], Step [4500/12500], Loss: 1.2621\n",
      "Epoch [1/10], Step [4600/12500], Loss: 1.2316\n",
      "Epoch [1/10], Step [4700/12500], Loss: 1.1749\n",
      "Epoch [1/10], Step [4800/12500], Loss: 1.3069\n",
      "Epoch [1/10], Step [4900/12500], Loss: 1.2587\n",
      "Epoch [1/10], Step [5000/12500], Loss: 1.2543\n",
      "Epoch [1/10], Step [5100/12500], Loss: 1.3301\n",
      "Epoch [1/10], Step [5200/12500], Loss: 1.2772\n",
      "Epoch [1/10], Step [5300/12500], Loss: 1.2296\n",
      "Epoch [1/10], Step [5400/12500], Loss: 1.2771\n",
      "Epoch [1/10], Step [5500/12500], Loss: 1.1702\n",
      "Epoch [1/10], Step [5600/12500], Loss: 1.1624\n",
      "Epoch [1/10], Step [5700/12500], Loss: 1.2412\n",
      "Epoch [1/10], Step [5800/12500], Loss: 1.1398\n",
      "Epoch [1/10], Step [5900/12500], Loss: 1.3112\n",
      "Epoch [1/10], Step [6000/12500], Loss: 1.0847\n",
      "Epoch [1/10], Step [6100/12500], Loss: 1.2163\n",
      "Epoch [1/10], Step [6200/12500], Loss: 1.1236\n",
      "Epoch [1/10], Step [6300/12500], Loss: 1.2035\n",
      "Epoch [1/10], Step [6400/12500], Loss: 1.1706\n",
      "Epoch [1/10], Step [6500/12500], Loss: 1.1742\n",
      "Epoch [1/10], Step [6600/12500], Loss: 1.1724\n",
      "Epoch [1/10], Step [6700/12500], Loss: 1.1920\n",
      "Epoch [1/10], Step [6800/12500], Loss: 1.0881\n",
      "Epoch [1/10], Step [6900/12500], Loss: 1.0705\n",
      "Epoch [1/10], Step [7000/12500], Loss: 1.1192\n",
      "Epoch [1/10], Step [7100/12500], Loss: 1.2522\n",
      "Epoch [1/10], Step [7200/12500], Loss: 1.1920\n",
      "Epoch [1/10], Step [7300/12500], Loss: 1.0284\n",
      "Epoch [1/10], Step [7400/12500], Loss: 1.1756\n",
      "Epoch [1/10], Step [7500/12500], Loss: 0.9916\n",
      "Epoch [1/10], Step [7600/12500], Loss: 1.0507\n",
      "Epoch [1/10], Step [7700/12500], Loss: 1.0594\n",
      "Epoch [1/10], Step [7800/12500], Loss: 1.1339\n",
      "Epoch [1/10], Step [7900/12500], Loss: 1.1621\n",
      "Epoch [1/10], Step [8000/12500], Loss: 1.0798\n",
      "Epoch [1/10], Step [8100/12500], Loss: 1.1066\n",
      "Epoch [1/10], Step [8200/12500], Loss: 1.1154\n",
      "Epoch [1/10], Step [8300/12500], Loss: 1.0198\n",
      "Epoch [1/10], Step [8400/12500], Loss: 1.1520\n",
      "Epoch [1/10], Step [8500/12500], Loss: 1.0674\n",
      "Epoch [1/10], Step [8600/12500], Loss: 1.0011\n",
      "Epoch [1/10], Step [8700/12500], Loss: 1.0785\n",
      "Epoch [1/10], Step [8800/12500], Loss: 1.0395\n",
      "Epoch [1/10], Step [8900/12500], Loss: 1.0681\n",
      "Epoch [1/10], Step [9000/12500], Loss: 1.0716\n",
      "Epoch [1/10], Step [9100/12500], Loss: 1.1613\n",
      "Epoch [1/10], Step [9200/12500], Loss: 1.0545\n",
      "Epoch [1/10], Step [9300/12500], Loss: 1.0824\n",
      "Epoch [1/10], Step [9400/12500], Loss: 1.0429\n",
      "Epoch [1/10], Step [9500/12500], Loss: 1.1039\n",
      "Epoch [1/10], Step [9600/12500], Loss: 1.0855\n",
      "Epoch [1/10], Step [9700/12500], Loss: 1.1058\n",
      "Epoch [1/10], Step [9800/12500], Loss: 1.1061\n",
      "Epoch [1/10], Step [9900/12500], Loss: 1.0171\n",
      "Epoch [1/10], Step [10000/12500], Loss: 0.9998\n",
      "Epoch [1/10], Step [10100/12500], Loss: 1.0861\n",
      "Epoch [1/10], Step [10200/12500], Loss: 1.0582\n",
      "Epoch [1/10], Step [10300/12500], Loss: 1.0482\n",
      "Epoch [1/10], Step [10400/12500], Loss: 1.0052\n",
      "Epoch [1/10], Step [10500/12500], Loss: 1.0081\n",
      "Epoch [1/10], Step [10600/12500], Loss: 0.9740\n",
      "Epoch [1/10], Step [10700/12500], Loss: 1.1131\n",
      "Epoch [1/10], Step [10800/12500], Loss: 1.0639\n",
      "Epoch [1/10], Step [10900/12500], Loss: 1.1394\n",
      "Epoch [1/10], Step [11000/12500], Loss: 1.0463\n",
      "Epoch [1/10], Step [11100/12500], Loss: 1.0226\n",
      "Epoch [1/10], Step [11200/12500], Loss: 1.0209\n",
      "Epoch [1/10], Step [11300/12500], Loss: 1.0758\n",
      "Epoch [1/10], Step [11400/12500], Loss: 0.9671\n",
      "Epoch [1/10], Step [11500/12500], Loss: 0.9971\n",
      "Epoch [1/10], Step [11600/12500], Loss: 0.9114\n",
      "Epoch [1/10], Step [11700/12500], Loss: 1.1284\n",
      "Epoch [1/10], Step [11800/12500], Loss: 1.0753\n",
      "Epoch [1/10], Step [11900/12500], Loss: 1.0031\n",
      "Epoch [1/10], Step [12000/12500], Loss: 0.9674\n",
      "Epoch [1/10], Step [12100/12500], Loss: 1.0182\n",
      "Epoch [1/10], Step [12200/12500], Loss: 0.9955\n",
      "Epoch [1/10], Step [12300/12500], Loss: 1.0853\n",
      "Epoch [1/10], Step [12400/12500], Loss: 1.0304\n",
      "Epoch [1/10], Step [12500/12500], Loss: 1.1022\n",
      "Epoch [2/10], Step [100/12500], Loss: 0.9300\n",
      "Epoch [2/10], Step [200/12500], Loss: 0.9007\n",
      "Epoch [2/10], Step [300/12500], Loss: 0.9245\n",
      "Epoch [2/10], Step [400/12500], Loss: 0.9267\n",
      "Epoch [2/10], Step [500/12500], Loss: 0.8688\n",
      "Epoch [2/10], Step [600/12500], Loss: 0.9521\n",
      "Epoch [2/10], Step [700/12500], Loss: 0.9000\n",
      "Epoch [2/10], Step [800/12500], Loss: 0.9117\n",
      "Epoch [2/10], Step [900/12500], Loss: 0.8624\n",
      "Epoch [2/10], Step [1000/12500], Loss: 0.9531\n",
      "Epoch [2/10], Step [1100/12500], Loss: 0.8758\n",
      "Epoch [2/10], Step [1200/12500], Loss: 0.8823\n",
      "Epoch [2/10], Step [1300/12500], Loss: 0.8974\n",
      "Epoch [2/10], Step [1400/12500], Loss: 0.8896\n",
      "Epoch [2/10], Step [1500/12500], Loss: 0.9641\n",
      "Epoch [2/10], Step [1600/12500], Loss: 0.9821\n",
      "Epoch [2/10], Step [1700/12500], Loss: 0.8387\n",
      "Epoch [2/10], Step [1800/12500], Loss: 0.9547\n",
      "Epoch [2/10], Step [1900/12500], Loss: 0.8589\n",
      "Epoch [2/10], Step [2000/12500], Loss: 0.8625\n",
      "Epoch [2/10], Step [2100/12500], Loss: 0.8371\n",
      "Epoch [2/10], Step [2200/12500], Loss: 0.9443\n",
      "Epoch [2/10], Step [2300/12500], Loss: 0.9516\n",
      "Epoch [2/10], Step [2400/12500], Loss: 0.7867\n",
      "Epoch [2/10], Step [2500/12500], Loss: 0.9418\n",
      "Epoch [2/10], Step [2600/12500], Loss: 1.0146\n",
      "Epoch [2/10], Step [2700/12500], Loss: 0.9003\n",
      "Epoch [2/10], Step [2800/12500], Loss: 0.9024\n",
      "Epoch [2/10], Step [2900/12500], Loss: 0.7461\n",
      "Epoch [2/10], Step [3000/12500], Loss: 0.9888\n",
      "Epoch [2/10], Step [3100/12500], Loss: 0.8708\n",
      "Epoch [2/10], Step [3200/12500], Loss: 0.9697\n",
      "Epoch [2/10], Step [3300/12500], Loss: 0.8940\n",
      "Epoch [2/10], Step [3400/12500], Loss: 0.9327\n",
      "Epoch [2/10], Step [3500/12500], Loss: 0.8186\n",
      "Epoch [2/10], Step [3600/12500], Loss: 0.8439\n",
      "Epoch [2/10], Step [3700/12500], Loss: 1.0019\n",
      "Epoch [2/10], Step [3800/12500], Loss: 0.9362\n",
      "Epoch [2/10], Step [3900/12500], Loss: 0.8842\n",
      "Epoch [2/10], Step [4000/12500], Loss: 0.9325\n",
      "Epoch [2/10], Step [4100/12500], Loss: 0.9827\n",
      "Epoch [2/10], Step [4200/12500], Loss: 0.8585\n",
      "Epoch [2/10], Step [4300/12500], Loss: 0.9253\n",
      "Epoch [2/10], Step [4400/12500], Loss: 1.0614\n",
      "Epoch [2/10], Step [4500/12500], Loss: 0.8869\n",
      "Epoch [2/10], Step [4600/12500], Loss: 0.9388\n",
      "Epoch [2/10], Step [4700/12500], Loss: 0.9311\n",
      "Epoch [2/10], Step [4800/12500], Loss: 0.8167\n",
      "Epoch [2/10], Step [4900/12500], Loss: 0.9267\n",
      "Epoch [2/10], Step [5000/12500], Loss: 0.9076\n",
      "Epoch [2/10], Step [5100/12500], Loss: 0.8963\n",
      "Epoch [2/10], Step [5200/12500], Loss: 0.8336\n",
      "Epoch [2/10], Step [5300/12500], Loss: 0.8812\n",
      "Epoch [2/10], Step [5400/12500], Loss: 0.8785\n",
      "Epoch [2/10], Step [5500/12500], Loss: 0.8213\n",
      "Epoch [2/10], Step [5600/12500], Loss: 0.8996\n",
      "Epoch [2/10], Step [5700/12500], Loss: 0.8598\n",
      "Epoch [2/10], Step [5800/12500], Loss: 0.9012\n",
      "Epoch [2/10], Step [5900/12500], Loss: 0.9129\n",
      "Epoch [2/10], Step [6000/12500], Loss: 0.9541\n",
      "Epoch [2/10], Step [6100/12500], Loss: 0.9209\n",
      "Epoch [2/10], Step [6200/12500], Loss: 0.8567\n",
      "Epoch [2/10], Step [6300/12500], Loss: 0.8045\n",
      "Epoch [2/10], Step [6400/12500], Loss: 0.8617\n",
      "Epoch [2/10], Step [6500/12500], Loss: 0.8218\n",
      "Epoch [2/10], Step [6600/12500], Loss: 0.9368\n",
      "Epoch [2/10], Step [6700/12500], Loss: 0.8000\n",
      "Epoch [2/10], Step [6800/12500], Loss: 0.9608\n",
      "Epoch [2/10], Step [6900/12500], Loss: 0.9159\n",
      "Epoch [2/10], Step [7000/12500], Loss: 0.8458\n",
      "Epoch [2/10], Step [7100/12500], Loss: 0.8697\n",
      "Epoch [2/10], Step [7200/12500], Loss: 0.9512\n",
      "Epoch [2/10], Step [7300/12500], Loss: 0.9275\n",
      "Epoch [2/10], Step [7400/12500], Loss: 0.8263\n",
      "Epoch [2/10], Step [7500/12500], Loss: 0.8308\n",
      "Epoch [2/10], Step [7600/12500], Loss: 0.9529\n",
      "Epoch [2/10], Step [7700/12500], Loss: 0.8682\n",
      "Epoch [2/10], Step [7800/12500], Loss: 0.7904\n",
      "Epoch [2/10], Step [7900/12500], Loss: 0.9535\n",
      "Epoch [2/10], Step [8000/12500], Loss: 0.9148\n",
      "Epoch [2/10], Step [8100/12500], Loss: 0.7883\n",
      "Epoch [2/10], Step [8200/12500], Loss: 0.8849\n",
      "Epoch [2/10], Step [8300/12500], Loss: 0.8220\n",
      "Epoch [2/10], Step [8400/12500], Loss: 0.8370\n",
      "Epoch [2/10], Step [8500/12500], Loss: 0.8783\n",
      "Epoch [2/10], Step [8600/12500], Loss: 0.9054\n",
      "Epoch [2/10], Step [8700/12500], Loss: 0.8927\n",
      "Epoch [2/10], Step [8800/12500], Loss: 0.9191\n",
      "Epoch [2/10], Step [8900/12500], Loss: 0.8314\n",
      "Epoch [2/10], Step [9000/12500], Loss: 0.9154\n",
      "Epoch [2/10], Step [9100/12500], Loss: 0.7569\n",
      "Epoch [2/10], Step [9200/12500], Loss: 0.9229\n",
      "Epoch [2/10], Step [9300/12500], Loss: 0.8303\n",
      "Epoch [2/10], Step [9400/12500], Loss: 0.9410\n",
      "Epoch [2/10], Step [9500/12500], Loss: 0.9611\n",
      "Epoch [2/10], Step [9600/12500], Loss: 0.8941\n",
      "Epoch [2/10], Step [9700/12500], Loss: 0.9129\n",
      "Epoch [2/10], Step [9800/12500], Loss: 0.9109\n",
      "Epoch [2/10], Step [9900/12500], Loss: 0.9103\n",
      "Epoch [2/10], Step [10000/12500], Loss: 0.9151\n",
      "Epoch [2/10], Step [10100/12500], Loss: 0.9467\n",
      "Epoch [2/10], Step [10200/12500], Loss: 0.8123\n",
      "Epoch [2/10], Step [10300/12500], Loss: 0.8198\n",
      "Epoch [2/10], Step [10400/12500], Loss: 0.8160\n",
      "Epoch [2/10], Step [10500/12500], Loss: 0.9125\n",
      "Epoch [2/10], Step [10600/12500], Loss: 0.8453\n",
      "Epoch [2/10], Step [10700/12500], Loss: 0.8601\n",
      "Epoch [2/10], Step [10800/12500], Loss: 0.8808\n",
      "Epoch [2/10], Step [10900/12500], Loss: 0.9648\n",
      "Epoch [2/10], Step [11000/12500], Loss: 1.0234\n",
      "Epoch [2/10], Step [11100/12500], Loss: 0.7288\n",
      "Epoch [2/10], Step [11200/12500], Loss: 0.8882\n",
      "Epoch [2/10], Step [11300/12500], Loss: 0.8838\n",
      "Epoch [2/10], Step [11400/12500], Loss: 0.9417\n",
      "Epoch [2/10], Step [11500/12500], Loss: 0.9392\n",
      "Epoch [2/10], Step [11600/12500], Loss: 0.9036\n",
      "Epoch [2/10], Step [11700/12500], Loss: 0.9888\n",
      "Epoch [2/10], Step [11800/12500], Loss: 0.8981\n",
      "Epoch [2/10], Step [11900/12500], Loss: 0.9476\n",
      "Epoch [2/10], Step [12000/12500], Loss: 0.8610\n",
      "Epoch [2/10], Step [12100/12500], Loss: 0.9015\n",
      "Epoch [2/10], Step [12200/12500], Loss: 1.1032\n",
      "Epoch [2/10], Step [12300/12500], Loss: 0.9254\n",
      "Epoch [2/10], Step [12400/12500], Loss: 0.8731\n",
      "Epoch [2/10], Step [12500/12500], Loss: 0.9155\n",
      "Epoch [3/10], Step [100/12500], Loss: 0.7700\n",
      "Epoch [3/10], Step [200/12500], Loss: 0.5849\n",
      "Epoch [3/10], Step [300/12500], Loss: 0.6980\n",
      "Epoch [3/10], Step [400/12500], Loss: 0.6369\n",
      "Epoch [3/10], Step [500/12500], Loss: 0.7176\n",
      "Epoch [3/10], Step [600/12500], Loss: 0.7003\n",
      "Epoch [3/10], Step [700/12500], Loss: 0.7509\n",
      "Epoch [3/10], Step [800/12500], Loss: 0.7415\n",
      "Epoch [3/10], Step [900/12500], Loss: 0.6612\n",
      "Epoch [3/10], Step [1000/12500], Loss: 0.7446\n",
      "Epoch [3/10], Step [1100/12500], Loss: 0.7193\n",
      "Epoch [3/10], Step [1200/12500], Loss: 0.6903\n",
      "Epoch [3/10], Step [1300/12500], Loss: 0.6685\n",
      "Epoch [3/10], Step [1400/12500], Loss: 0.8280\n",
      "Epoch [3/10], Step [1500/12500], Loss: 0.6784\n",
      "Epoch [3/10], Step [1600/12500], Loss: 0.7821\n",
      "Epoch [3/10], Step [1700/12500], Loss: 0.7885\n",
      "Epoch [3/10], Step [1800/12500], Loss: 0.6803\n",
      "Epoch [3/10], Step [1900/12500], Loss: 0.7449\n",
      "Epoch [3/10], Step [2000/12500], Loss: 0.6661\n",
      "Epoch [3/10], Step [2100/12500], Loss: 0.6752\n",
      "Epoch [3/10], Step [2200/12500], Loss: 0.6242\n",
      "Epoch [3/10], Step [2300/12500], Loss: 0.7947\n",
      "Epoch [3/10], Step [2400/12500], Loss: 0.7212\n",
      "Epoch [3/10], Step [2500/12500], Loss: 0.6824\n",
      "Epoch [3/10], Step [2600/12500], Loss: 0.7692\n",
      "Epoch [3/10], Step [2700/12500], Loss: 0.7259\n",
      "Epoch [3/10], Step [2800/12500], Loss: 0.7447\n",
      "Epoch [3/10], Step [2900/12500], Loss: 0.7883\n",
      "Epoch [3/10], Step [3000/12500], Loss: 0.7242\n",
      "Epoch [3/10], Step [3100/12500], Loss: 0.7436\n",
      "Epoch [3/10], Step [3200/12500], Loss: 0.8360\n",
      "Epoch [3/10], Step [3300/12500], Loss: 0.7177\n",
      "Epoch [3/10], Step [3400/12500], Loss: 0.6713\n",
      "Epoch [3/10], Step [3500/12500], Loss: 0.6644\n",
      "Epoch [3/10], Step [3600/12500], Loss: 0.7254\n",
      "Epoch [3/10], Step [3700/12500], Loss: 0.6756\n",
      "Epoch [3/10], Step [3800/12500], Loss: 0.7413\n",
      "Epoch [3/10], Step [3900/12500], Loss: 0.7236\n",
      "Epoch [3/10], Step [4000/12500], Loss: 0.7046\n",
      "Epoch [3/10], Step [4100/12500], Loss: 0.7452\n",
      "Epoch [3/10], Step [4200/12500], Loss: 0.6670\n",
      "Epoch [3/10], Step [4300/12500], Loss: 0.7708\n",
      "Epoch [3/10], Step [4400/12500], Loss: 0.7119\n",
      "Epoch [3/10], Step [4500/12500], Loss: 0.6534\n",
      "Epoch [3/10], Step [4600/12500], Loss: 0.8118\n",
      "Epoch [3/10], Step [4700/12500], Loss: 0.7761\n",
      "Epoch [3/10], Step [4800/12500], Loss: 0.7755\n",
      "Epoch [3/10], Step [4900/12500], Loss: 0.6742\n",
      "Epoch [3/10], Step [5000/12500], Loss: 0.8064\n",
      "Epoch [3/10], Step [5100/12500], Loss: 0.7086\n",
      "Epoch [3/10], Step [5200/12500], Loss: 0.7238\n",
      "Epoch [3/10], Step [5300/12500], Loss: 0.7546\n",
      "Epoch [3/10], Step [5400/12500], Loss: 0.6987\n",
      "Epoch [3/10], Step [5500/12500], Loss: 0.7361\n",
      "Epoch [3/10], Step [5600/12500], Loss: 0.7583\n",
      "Epoch [3/10], Step [5700/12500], Loss: 0.7053\n",
      "Epoch [3/10], Step [5800/12500], Loss: 0.7906\n",
      "Epoch [3/10], Step [5900/12500], Loss: 0.8050\n",
      "Epoch [3/10], Step [6000/12500], Loss: 0.6491\n",
      "Epoch [3/10], Step [6100/12500], Loss: 0.7483\n",
      "Epoch [3/10], Step [6200/12500], Loss: 0.7728\n",
      "Epoch [3/10], Step [6300/12500], Loss: 0.7300\n",
      "Epoch [3/10], Step [6400/12500], Loss: 0.7581\n",
      "Epoch [3/10], Step [6500/12500], Loss: 0.7474\n",
      "Epoch [3/10], Step [6600/12500], Loss: 0.7579\n",
      "Epoch [3/10], Step [6700/12500], Loss: 0.7759\n",
      "Epoch [3/10], Step [6800/12500], Loss: 0.7568\n",
      "Epoch [3/10], Step [6900/12500], Loss: 0.6661\n",
      "Epoch [3/10], Step [7000/12500], Loss: 0.6754\n",
      "Epoch [3/10], Step [7100/12500], Loss: 0.7237\n",
      "Epoch [3/10], Step [7200/12500], Loss: 0.7305\n",
      "Epoch [3/10], Step [7300/12500], Loss: 0.6913\n",
      "Epoch [3/10], Step [7400/12500], Loss: 0.8512\n",
      "Epoch [3/10], Step [7500/12500], Loss: 0.8073\n",
      "Epoch [3/10], Step [7600/12500], Loss: 0.7564\n",
      "Epoch [3/10], Step [7700/12500], Loss: 0.7230\n",
      "Epoch [3/10], Step [7800/12500], Loss: 0.7939\n",
      "Epoch [3/10], Step [7900/12500], Loss: 0.6945\n",
      "Epoch [3/10], Step [8000/12500], Loss: 0.6828\n",
      "Epoch [3/10], Step [8100/12500], Loss: 0.6687\n",
      "Epoch [3/10], Step [8200/12500], Loss: 0.8379\n",
      "Epoch [3/10], Step [8300/12500], Loss: 0.7290\n",
      "Epoch [3/10], Step [8400/12500], Loss: 0.7723\n",
      "Epoch [3/10], Step [8500/12500], Loss: 0.6987\n",
      "Epoch [3/10], Step [8600/12500], Loss: 0.8387\n",
      "Epoch [3/10], Step [8700/12500], Loss: 0.6901\n",
      "Epoch [3/10], Step [8800/12500], Loss: 0.7477\n",
      "Epoch [3/10], Step [8900/12500], Loss: 0.6733\n",
      "Epoch [3/10], Step [9000/12500], Loss: 0.8129\n",
      "Epoch [3/10], Step [9100/12500], Loss: 0.6807\n",
      "Epoch [3/10], Step [9200/12500], Loss: 0.7084\n",
      "Epoch [3/10], Step [9300/12500], Loss: 0.7443\n",
      "Epoch [3/10], Step [9400/12500], Loss: 0.8113\n",
      "Epoch [3/10], Step [9500/12500], Loss: 0.7816\n",
      "Epoch [3/10], Step [9600/12500], Loss: 0.7252\n",
      "Epoch [3/10], Step [9700/12500], Loss: 0.6837\n",
      "Epoch [3/10], Step [9800/12500], Loss: 0.7997\n",
      "Epoch [3/10], Step [9900/12500], Loss: 0.7164\n",
      "Epoch [3/10], Step [10000/12500], Loss: 0.7857\n",
      "Epoch [3/10], Step [10100/12500], Loss: 0.6421\n",
      "Epoch [3/10], Step [10200/12500], Loss: 0.7873\n",
      "Epoch [3/10], Step [10300/12500], Loss: 0.8968\n",
      "Epoch [3/10], Step [10400/12500], Loss: 0.7997\n",
      "Epoch [3/10], Step [10500/12500], Loss: 0.7266\n",
      "Epoch [3/10], Step [10600/12500], Loss: 0.7718\n",
      "Epoch [3/10], Step [10700/12500], Loss: 0.7899\n",
      "Epoch [3/10], Step [10800/12500], Loss: 0.7936\n",
      "Epoch [3/10], Step [10900/12500], Loss: 0.7831\n",
      "Epoch [3/10], Step [11000/12500], Loss: 0.6599\n",
      "Epoch [3/10], Step [11100/12500], Loss: 0.7465\n",
      "Epoch [3/10], Step [11200/12500], Loss: 0.7634\n",
      "Epoch [3/10], Step [11300/12500], Loss: 0.8170\n",
      "Epoch [3/10], Step [11400/12500], Loss: 0.7465\n",
      "Epoch [3/10], Step [11500/12500], Loss: 0.7699\n",
      "Epoch [3/10], Step [11600/12500], Loss: 0.7728\n",
      "Epoch [3/10], Step [11700/12500], Loss: 0.8075\n",
      "Epoch [3/10], Step [11800/12500], Loss: 0.8514\n",
      "Epoch [3/10], Step [11900/12500], Loss: 0.7903\n",
      "Epoch [3/10], Step [12000/12500], Loss: 0.6950\n",
      "Epoch [3/10], Step [12100/12500], Loss: 0.7770\n",
      "Epoch [3/10], Step [12200/12500], Loss: 0.7281\n",
      "Epoch [3/10], Step [12300/12500], Loss: 0.7114\n",
      "Epoch [3/10], Step [12400/12500], Loss: 0.7991\n",
      "Epoch [3/10], Step [12500/12500], Loss: 0.7172\n",
      "Epoch [4/10], Step [100/12500], Loss: 0.4989\n",
      "Epoch [4/10], Step [200/12500], Loss: 0.5702\n",
      "Epoch [4/10], Step [300/12500], Loss: 0.5791\n",
      "Epoch [4/10], Step [400/12500], Loss: 0.5132\n",
      "Epoch [4/10], Step [500/12500], Loss: 0.6004\n",
      "Epoch [4/10], Step [600/12500], Loss: 0.4800\n",
      "Epoch [4/10], Step [700/12500], Loss: 0.5729\n",
      "Epoch [4/10], Step [800/12500], Loss: 0.6064\n",
      "Epoch [4/10], Step [900/12500], Loss: 0.5590\n",
      "Epoch [4/10], Step [1000/12500], Loss: 0.5593\n",
      "Epoch [4/10], Step [1100/12500], Loss: 0.5112\n",
      "Epoch [4/10], Step [1200/12500], Loss: 0.5487\n",
      "Epoch [4/10], Step [1300/12500], Loss: 0.5390\n",
      "Epoch [4/10], Step [1400/12500], Loss: 0.5214\n",
      "Epoch [4/10], Step [1500/12500], Loss: 0.6456\n",
      "Epoch [4/10], Step [1600/12500], Loss: 0.5625\n",
      "Epoch [4/10], Step [1700/12500], Loss: 0.7336\n",
      "Epoch [4/10], Step [1800/12500], Loss: 0.5354\n",
      "Epoch [4/10], Step [1900/12500], Loss: 0.5489\n",
      "Epoch [4/10], Step [2000/12500], Loss: 0.6127\n",
      "Epoch [4/10], Step [2100/12500], Loss: 0.5132\n",
      "Epoch [4/10], Step [2200/12500], Loss: 0.5607\n",
      "Epoch [4/10], Step [2300/12500], Loss: 0.5444\n",
      "Epoch [4/10], Step [2400/12500], Loss: 0.5867\n",
      "Epoch [4/10], Step [2500/12500], Loss: 0.5577\n",
      "Epoch [4/10], Step [2600/12500], Loss: 0.5832\n",
      "Epoch [4/10], Step [2700/12500], Loss: 0.5696\n",
      "Epoch [4/10], Step [2800/12500], Loss: 0.6118\n",
      "Epoch [4/10], Step [2900/12500], Loss: 0.5684\n",
      "Epoch [4/10], Step [3000/12500], Loss: 0.5445\n",
      "Epoch [4/10], Step [3100/12500], Loss: 0.6204\n",
      "Epoch [4/10], Step [3200/12500], Loss: 0.5889\n",
      "Epoch [4/10], Step [3300/12500], Loss: 0.6161\n",
      "Epoch [4/10], Step [3400/12500], Loss: 0.6573\n",
      "Epoch [4/10], Step [3500/12500], Loss: 0.6859\n",
      "Epoch [4/10], Step [3600/12500], Loss: 0.5408\n",
      "Epoch [4/10], Step [3700/12500], Loss: 0.6163\n",
      "Epoch [4/10], Step [3800/12500], Loss: 0.6074\n",
      "Epoch [4/10], Step [3900/12500], Loss: 0.6484\n",
      "Epoch [4/10], Step [4000/12500], Loss: 0.6098\n",
      "Epoch [4/10], Step [4100/12500], Loss: 0.5429\n",
      "Epoch [4/10], Step [4200/12500], Loss: 0.6313\n",
      "Epoch [4/10], Step [4300/12500], Loss: 0.6602\n",
      "Epoch [4/10], Step [4400/12500], Loss: 0.6528\n",
      "Epoch [4/10], Step [4500/12500], Loss: 0.5451\n",
      "Epoch [4/10], Step [4600/12500], Loss: 0.5860\n",
      "Epoch [4/10], Step [4700/12500], Loss: 0.6411\n",
      "Epoch [4/10], Step [4800/12500], Loss: 0.5747\n",
      "Epoch [4/10], Step [4900/12500], Loss: 0.5559\n",
      "Epoch [4/10], Step [5000/12500], Loss: 0.6673\n",
      "Epoch [4/10], Step [5100/12500], Loss: 0.5953\n",
      "Epoch [4/10], Step [5200/12500], Loss: 0.6128\n",
      "Epoch [4/10], Step [5300/12500], Loss: 0.5918\n",
      "Epoch [4/10], Step [5400/12500], Loss: 0.5825\n",
      "Epoch [4/10], Step [5500/12500], Loss: 0.6265\n",
      "Epoch [4/10], Step [5600/12500], Loss: 0.5085\n",
      "Epoch [4/10], Step [5700/12500], Loss: 0.5227\n",
      "Epoch [4/10], Step [5800/12500], Loss: 0.6247\n",
      "Epoch [4/10], Step [5900/12500], Loss: 0.6593\n",
      "Epoch [4/10], Step [6000/12500], Loss: 0.6820\n",
      "Epoch [4/10], Step [6100/12500], Loss: 0.4967\n",
      "Epoch [4/10], Step [6200/12500], Loss: 0.5983\n",
      "Epoch [4/10], Step [6300/12500], Loss: 0.5086\n",
      "Epoch [4/10], Step [6400/12500], Loss: 0.5419\n",
      "Epoch [4/10], Step [6500/12500], Loss: 0.5424\n",
      "Epoch [4/10], Step [6600/12500], Loss: 0.5707\n",
      "Epoch [4/10], Step [6700/12500], Loss: 0.5758\n",
      "Epoch [4/10], Step [6800/12500], Loss: 0.6665\n",
      "Epoch [4/10], Step [6900/12500], Loss: 0.6192\n",
      "Epoch [4/10], Step [7000/12500], Loss: 0.6599\n",
      "Epoch [4/10], Step [7100/12500], Loss: 0.6537\n",
      "Epoch [4/10], Step [7200/12500], Loss: 0.6720\n",
      "Epoch [4/10], Step [7300/12500], Loss: 0.5258\n",
      "Epoch [4/10], Step [7400/12500], Loss: 0.6632\n",
      "Epoch [4/10], Step [7500/12500], Loss: 0.6578\n",
      "Epoch [4/10], Step [7600/12500], Loss: 0.5563\n",
      "Epoch [4/10], Step [7700/12500], Loss: 0.5459\n",
      "Epoch [4/10], Step [7800/12500], Loss: 0.5686\n",
      "Epoch [4/10], Step [7900/12500], Loss: 0.6239\n",
      "Epoch [4/10], Step [8000/12500], Loss: 0.6653\n",
      "Epoch [4/10], Step [8100/12500], Loss: 0.5206\n",
      "Epoch [4/10], Step [8200/12500], Loss: 0.6246\n",
      "Epoch [4/10], Step [8300/12500], Loss: 0.5323\n",
      "Epoch [4/10], Step [8400/12500], Loss: 0.7399\n",
      "Epoch [4/10], Step [8500/12500], Loss: 0.6654\n",
      "Epoch [4/10], Step [8600/12500], Loss: 0.5921\n",
      "Epoch [4/10], Step [8700/12500], Loss: 0.5645\n",
      "Epoch [4/10], Step [8800/12500], Loss: 0.5451\n",
      "Epoch [4/10], Step [8900/12500], Loss: 0.6513\n",
      "Epoch [4/10], Step [9000/12500], Loss: 0.6658\n",
      "Epoch [4/10], Step [9100/12500], Loss: 0.5653\n",
      "Epoch [4/10], Step [9200/12500], Loss: 0.5302\n",
      "Epoch [4/10], Step [9300/12500], Loss: 0.7191\n",
      "Epoch [4/10], Step [9400/12500], Loss: 0.6275\n",
      "Epoch [4/10], Step [9500/12500], Loss: 0.6380\n",
      "Epoch [4/10], Step [9600/12500], Loss: 0.6334\n",
      "Epoch [4/10], Step [9700/12500], Loss: 0.6520\n",
      "Epoch [4/10], Step [9800/12500], Loss: 0.7694\n",
      "Epoch [4/10], Step [9900/12500], Loss: 0.5848\n",
      "Epoch [4/10], Step [10000/12500], Loss: 0.6379\n",
      "Epoch [4/10], Step [10100/12500], Loss: 0.6496\n",
      "Epoch [4/10], Step [10200/12500], Loss: 0.7420\n",
      "Epoch [4/10], Step [10300/12500], Loss: 0.6708\n",
      "Epoch [4/10], Step [10400/12500], Loss: 0.6958\n",
      "Epoch [4/10], Step [10500/12500], Loss: 0.6116\n",
      "Epoch [4/10], Step [10600/12500], Loss: 0.5154\n",
      "Epoch [4/10], Step [10700/12500], Loss: 0.5601\n",
      "Epoch [4/10], Step [10800/12500], Loss: 0.7173\n",
      "Epoch [4/10], Step [10900/12500], Loss: 0.6751\n",
      "Epoch [4/10], Step [11000/12500], Loss: 0.7067\n",
      "Epoch [4/10], Step [11100/12500], Loss: 0.6174\n",
      "Epoch [4/10], Step [11200/12500], Loss: 0.5535\n",
      "Epoch [4/10], Step [11300/12500], Loss: 0.6672\n",
      "Epoch [4/10], Step [11400/12500], Loss: 0.6922\n",
      "Epoch [4/10], Step [11500/12500], Loss: 0.6507\n",
      "Epoch [4/10], Step [11600/12500], Loss: 0.6883\n",
      "Epoch [4/10], Step [11700/12500], Loss: 0.6449\n",
      "Epoch [4/10], Step [11800/12500], Loss: 0.6774\n",
      "Epoch [4/10], Step [11900/12500], Loss: 0.6606\n",
      "Epoch [4/10], Step [12000/12500], Loss: 0.6194\n",
      "Epoch [4/10], Step [12100/12500], Loss: 0.6804\n",
      "Epoch [4/10], Step [12200/12500], Loss: 0.6458\n",
      "Epoch [4/10], Step [12300/12500], Loss: 0.6808\n",
      "Epoch [4/10], Step [12400/12500], Loss: 0.6542\n",
      "Epoch [4/10], Step [12500/12500], Loss: 0.6305\n",
      "Epoch [5/10], Step [100/12500], Loss: 0.4050\n",
      "Epoch [5/10], Step [200/12500], Loss: 0.4871\n",
      "Epoch [5/10], Step [300/12500], Loss: 0.4883\n",
      "Epoch [5/10], Step [400/12500], Loss: 0.3935\n",
      "Epoch [5/10], Step [500/12500], Loss: 0.3889\n",
      "Epoch [5/10], Step [600/12500], Loss: 0.4989\n",
      "Epoch [5/10], Step [700/12500], Loss: 0.4186\n",
      "Epoch [5/10], Step [800/12500], Loss: 0.4396\n",
      "Epoch [5/10], Step [900/12500], Loss: 0.3879\n",
      "Epoch [5/10], Step [1000/12500], Loss: 0.3879\n",
      "Epoch [5/10], Step [1100/12500], Loss: 0.4666\n",
      "Epoch [5/10], Step [1200/12500], Loss: 0.4153\n",
      "Epoch [5/10], Step [1300/12500], Loss: 0.4642\n",
      "Epoch [5/10], Step [1400/12500], Loss: 0.4623\n",
      "Epoch [5/10], Step [1500/12500], Loss: 0.4344\n",
      "Epoch [5/10], Step [1600/12500], Loss: 0.4929\n",
      "Epoch [5/10], Step [1700/12500], Loss: 0.3891\n",
      "Epoch [5/10], Step [1800/12500], Loss: 0.4252\n",
      "Epoch [5/10], Step [1900/12500], Loss: 0.4902\n",
      "Epoch [5/10], Step [2000/12500], Loss: 0.4727\n",
      "Epoch [5/10], Step [2100/12500], Loss: 0.5951\n",
      "Epoch [5/10], Step [2200/12500], Loss: 0.4439\n",
      "Epoch [5/10], Step [2300/12500], Loss: 0.4667\n",
      "Epoch [5/10], Step [2400/12500], Loss: 0.3765\n",
      "Epoch [5/10], Step [2500/12500], Loss: 0.4252\n",
      "Epoch [5/10], Step [2600/12500], Loss: 0.3671\n",
      "Epoch [5/10], Step [2700/12500], Loss: 0.5025\n",
      "Epoch [5/10], Step [2800/12500], Loss: 0.5580\n",
      "Epoch [5/10], Step [2900/12500], Loss: 0.4026\n",
      "Epoch [5/10], Step [3000/12500], Loss: 0.5600\n",
      "Epoch [5/10], Step [3100/12500], Loss: 0.4536\n",
      "Epoch [5/10], Step [3200/12500], Loss: 0.4282\n",
      "Epoch [5/10], Step [3300/12500], Loss: 0.4177\n",
      "Epoch [5/10], Step [3400/12500], Loss: 0.4590\n",
      "Epoch [5/10], Step [3500/12500], Loss: 0.5463\n",
      "Epoch [5/10], Step [3600/12500], Loss: 0.5710\n",
      "Epoch [5/10], Step [3700/12500], Loss: 0.4445\n",
      "Epoch [5/10], Step [3800/12500], Loss: 0.4252\n",
      "Epoch [5/10], Step [3900/12500], Loss: 0.4818\n",
      "Epoch [5/10], Step [4000/12500], Loss: 0.5153\n",
      "Epoch [5/10], Step [4100/12500], Loss: 0.4786\n",
      "Epoch [5/10], Step [4200/12500], Loss: 0.3852\n",
      "Epoch [5/10], Step [4300/12500], Loss: 0.4132\n",
      "Epoch [5/10], Step [4400/12500], Loss: 0.5185\n",
      "Epoch [5/10], Step [4500/12500], Loss: 0.5362\n",
      "Epoch [5/10], Step [4600/12500], Loss: 0.4855\n",
      "Epoch [5/10], Step [4700/12500], Loss: 0.4799\n",
      "Epoch [5/10], Step [4800/12500], Loss: 0.4252\n",
      "Epoch [5/10], Step [4900/12500], Loss: 0.4641\n",
      "Epoch [5/10], Step [5000/12500], Loss: 0.5321\n",
      "Epoch [5/10], Step [5100/12500], Loss: 0.5800\n",
      "Epoch [5/10], Step [5200/12500], Loss: 0.5311\n",
      "Epoch [5/10], Step [5300/12500], Loss: 0.5046\n",
      "Epoch [5/10], Step [5400/12500], Loss: 0.4766\n",
      "Epoch [5/10], Step [5500/12500], Loss: 0.4460\n",
      "Epoch [5/10], Step [5600/12500], Loss: 0.4963\n",
      "Epoch [5/10], Step [5700/12500], Loss: 0.4387\n",
      "Epoch [5/10], Step [5800/12500], Loss: 0.4899\n",
      "Epoch [5/10], Step [5900/12500], Loss: 0.4964\n",
      "Epoch [5/10], Step [6000/12500], Loss: 0.5778\n",
      "Epoch [5/10], Step [6100/12500], Loss: 0.5891\n",
      "Epoch [5/10], Step [6200/12500], Loss: 0.5505\n",
      "Epoch [5/10], Step [6300/12500], Loss: 0.5336\n",
      "Epoch [5/10], Step [6400/12500], Loss: 0.5255\n",
      "Epoch [5/10], Step [6500/12500], Loss: 0.5572\n",
      "Epoch [5/10], Step [6600/12500], Loss: 0.4851\n",
      "Epoch [5/10], Step [6700/12500], Loss: 0.4718\n",
      "Epoch [5/10], Step [6800/12500], Loss: 0.5798\n",
      "Epoch [5/10], Step [6900/12500], Loss: 0.4928\n",
      "Epoch [5/10], Step [7000/12500], Loss: 0.6222\n",
      "Epoch [5/10], Step [7100/12500], Loss: 0.4537\n",
      "Epoch [5/10], Step [7200/12500], Loss: 0.5608\n",
      "Epoch [5/10], Step [7300/12500], Loss: 0.4872\n",
      "Epoch [5/10], Step [7400/12500], Loss: 0.4708\n",
      "Epoch [5/10], Step [7500/12500], Loss: 0.5230\n",
      "Epoch [5/10], Step [7600/12500], Loss: 0.4377\n",
      "Epoch [5/10], Step [7700/12500], Loss: 0.4628\n",
      "Epoch [5/10], Step [7800/12500], Loss: 0.5106\n",
      "Epoch [5/10], Step [7900/12500], Loss: 0.5314\n",
      "Epoch [5/10], Step [8000/12500], Loss: 0.4988\n",
      "Epoch [5/10], Step [8100/12500], Loss: 0.4992\n",
      "Epoch [5/10], Step [8200/12500], Loss: 0.4297\n",
      "Epoch [5/10], Step [8300/12500], Loss: 0.4547\n",
      "Epoch [5/10], Step [8400/12500], Loss: 0.4843\n",
      "Epoch [5/10], Step [8500/12500], Loss: 0.5069\n",
      "Epoch [5/10], Step [8600/12500], Loss: 0.6213\n",
      "Epoch [5/10], Step [8700/12500], Loss: 0.5321\n",
      "Epoch [5/10], Step [8800/12500], Loss: 0.5025\n",
      "Epoch [5/10], Step [8900/12500], Loss: 0.3564\n",
      "Epoch [5/10], Step [9000/12500], Loss: 0.5070\n",
      "Epoch [5/10], Step [9100/12500], Loss: 0.4867\n",
      "Epoch [5/10], Step [9200/12500], Loss: 0.4432\n",
      "Epoch [5/10], Step [9300/12500], Loss: 0.4869\n",
      "Epoch [5/10], Step [9400/12500], Loss: 0.5133\n",
      "Epoch [5/10], Step [9500/12500], Loss: 0.5801\n",
      "Epoch [5/10], Step [9600/12500], Loss: 0.5600\n",
      "Epoch [5/10], Step [9700/12500], Loss: 0.4924\n",
      "Epoch [5/10], Step [9800/12500], Loss: 0.4877\n",
      "Epoch [5/10], Step [9900/12500], Loss: 0.6367\n",
      "Epoch [5/10], Step [10000/12500], Loss: 0.4916\n",
      "Epoch [5/10], Step [10100/12500], Loss: 0.5503\n",
      "Epoch [5/10], Step [10200/12500], Loss: 0.6095\n",
      "Epoch [5/10], Step [10300/12500], Loss: 0.5102\n",
      "Epoch [5/10], Step [10400/12500], Loss: 0.6180\n",
      "Epoch [5/10], Step [10500/12500], Loss: 0.5630\n",
      "Epoch [5/10], Step [10600/12500], Loss: 0.4525\n",
      "Epoch [5/10], Step [10700/12500], Loss: 0.5510\n",
      "Epoch [5/10], Step [10800/12500], Loss: 0.4889\n",
      "Epoch [5/10], Step [10900/12500], Loss: 0.5237\n",
      "Epoch [5/10], Step [11000/12500], Loss: 0.5885\n",
      "Epoch [5/10], Step [11100/12500], Loss: 0.4915\n",
      "Epoch [5/10], Step [11200/12500], Loss: 0.5750\n",
      "Epoch [5/10], Step [11300/12500], Loss: 0.6240\n",
      "Epoch [5/10], Step [11400/12500], Loss: 0.5262\n",
      "Epoch [5/10], Step [11500/12500], Loss: 0.4972\n",
      "Epoch [5/10], Step [11600/12500], Loss: 0.5373\n",
      "Epoch [5/10], Step [11700/12500], Loss: 0.5543\n",
      "Epoch [5/10], Step [11800/12500], Loss: 0.5294\n",
      "Epoch [5/10], Step [11900/12500], Loss: 0.5677\n",
      "Epoch [5/10], Step [12000/12500], Loss: 0.4707\n",
      "Epoch [5/10], Step [12100/12500], Loss: 0.5189\n",
      "Epoch [5/10], Step [12200/12500], Loss: 0.4464\n",
      "Epoch [5/10], Step [12300/12500], Loss: 0.5473\n",
      "Epoch [5/10], Step [12400/12500], Loss: 0.5036\n",
      "Epoch [5/10], Step [12500/12500], Loss: 0.5805\n",
      "Epoch [6/10], Step [100/12500], Loss: 0.3687\n",
      "Epoch [6/10], Step [200/12500], Loss: 0.3335\n",
      "Epoch [6/10], Step [300/12500], Loss: 0.3467\n",
      "Epoch [6/10], Step [400/12500], Loss: 0.3702\n",
      "Epoch [6/10], Step [500/12500], Loss: 0.2499\n",
      "Epoch [6/10], Step [600/12500], Loss: 0.3413\n",
      "Epoch [6/10], Step [700/12500], Loss: 0.3154\n",
      "Epoch [6/10], Step [800/12500], Loss: 0.3462\n",
      "Epoch [6/10], Step [900/12500], Loss: 0.2599\n",
      "Epoch [6/10], Step [1000/12500], Loss: 0.3249\n",
      "Epoch [6/10], Step [1100/12500], Loss: 0.3571\n",
      "Epoch [6/10], Step [1200/12500], Loss: 0.3537\n",
      "Epoch [6/10], Step [1300/12500], Loss: 0.2893\n",
      "Epoch [6/10], Step [1400/12500], Loss: 0.3240\n",
      "Epoch [6/10], Step [1500/12500], Loss: 0.3223\n",
      "Epoch [6/10], Step [1600/12500], Loss: 0.3558\n",
      "Epoch [6/10], Step [1700/12500], Loss: 0.4181\n",
      "Epoch [6/10], Step [1800/12500], Loss: 0.3208\n",
      "Epoch [6/10], Step [1900/12500], Loss: 0.3302\n",
      "Epoch [6/10], Step [2000/12500], Loss: 0.3297\n",
      "Epoch [6/10], Step [2100/12500], Loss: 0.2818\n",
      "Epoch [6/10], Step [2200/12500], Loss: 0.3425\n",
      "Epoch [6/10], Step [2300/12500], Loss: 0.4252\n",
      "Epoch [6/10], Step [2400/12500], Loss: 0.2861\n",
      "Epoch [6/10], Step [2500/12500], Loss: 0.3955\n",
      "Epoch [6/10], Step [2600/12500], Loss: 0.3256\n",
      "Epoch [6/10], Step [2700/12500], Loss: 0.3355\n",
      "Epoch [6/10], Step [2800/12500], Loss: 0.3599\n",
      "Epoch [6/10], Step [2900/12500], Loss: 0.3479\n",
      "Epoch [6/10], Step [3000/12500], Loss: 0.4069\n",
      "Epoch [6/10], Step [3100/12500], Loss: 0.3925\n",
      "Epoch [6/10], Step [3200/12500], Loss: 0.3327\n",
      "Epoch [6/10], Step [3300/12500], Loss: 0.3519\n",
      "Epoch [6/10], Step [3400/12500], Loss: 0.4433\n",
      "Epoch [6/10], Step [3500/12500], Loss: 0.3876\n",
      "Epoch [6/10], Step [3600/12500], Loss: 0.3494\n",
      "Epoch [6/10], Step [3700/12500], Loss: 0.4065\n",
      "Epoch [6/10], Step [3800/12500], Loss: 0.4117\n",
      "Epoch [6/10], Step [3900/12500], Loss: 0.4873\n",
      "Epoch [6/10], Step [4000/12500], Loss: 0.4762\n",
      "Epoch [6/10], Step [4100/12500], Loss: 0.3433\n",
      "Epoch [6/10], Step [4200/12500], Loss: 0.3385\n",
      "Epoch [6/10], Step [4300/12500], Loss: 0.2781\n",
      "Epoch [6/10], Step [4400/12500], Loss: 0.2966\n",
      "Epoch [6/10], Step [4500/12500], Loss: 0.3340\n",
      "Epoch [6/10], Step [4600/12500], Loss: 0.3806\n",
      "Epoch [6/10], Step [4700/12500], Loss: 0.4005\n",
      "Epoch [6/10], Step [4800/12500], Loss: 0.4090\n",
      "Epoch [6/10], Step [4900/12500], Loss: 0.4196\n",
      "Epoch [6/10], Step [5000/12500], Loss: 0.4375\n",
      "Epoch [6/10], Step [5100/12500], Loss: 0.4549\n",
      "Epoch [6/10], Step [5200/12500], Loss: 0.3219\n",
      "Epoch [6/10], Step [5300/12500], Loss: 0.3495\n",
      "Epoch [6/10], Step [5400/12500], Loss: 0.3602\n",
      "Epoch [6/10], Step [5500/12500], Loss: 0.3727\n",
      "Epoch [6/10], Step [5600/12500], Loss: 0.4748\n",
      "Epoch [6/10], Step [5700/12500], Loss: 0.3892\n",
      "Epoch [6/10], Step [5800/12500], Loss: 0.3806\n",
      "Epoch [6/10], Step [5900/12500], Loss: 0.2887\n",
      "Epoch [6/10], Step [6000/12500], Loss: 0.3958\n",
      "Epoch [6/10], Step [6100/12500], Loss: 0.4445\n",
      "Epoch [6/10], Step [6200/12500], Loss: 0.4785\n",
      "Epoch [6/10], Step [6300/12500], Loss: 0.3884\n",
      "Epoch [6/10], Step [6400/12500], Loss: 0.4570\n",
      "Epoch [6/10], Step [6500/12500], Loss: 0.4699\n",
      "Epoch [6/10], Step [6600/12500], Loss: 0.3952\n",
      "Epoch [6/10], Step [6700/12500], Loss: 0.4013\n",
      "Epoch [6/10], Step [6800/12500], Loss: 0.3774\n",
      "Epoch [6/10], Step [6900/12500], Loss: 0.3550\n",
      "Epoch [6/10], Step [7000/12500], Loss: 0.3688\n",
      "Epoch [6/10], Step [7100/12500], Loss: 0.4154\n",
      "Epoch [6/10], Step [7200/12500], Loss: 0.5240\n",
      "Epoch [6/10], Step [7300/12500], Loss: 0.3652\n",
      "Epoch [6/10], Step [7400/12500], Loss: 0.3536\n",
      "Epoch [6/10], Step [7500/12500], Loss: 0.4546\n",
      "Epoch [6/10], Step [7600/12500], Loss: 0.3748\n",
      "Epoch [6/10], Step [7700/12500], Loss: 0.4648\n",
      "Epoch [6/10], Step [7800/12500], Loss: 0.3086\n",
      "Epoch [6/10], Step [7900/12500], Loss: 0.4303\n",
      "Epoch [6/10], Step [8000/12500], Loss: 0.4926\n",
      "Epoch [6/10], Step [8100/12500], Loss: 0.3718\n",
      "Epoch [6/10], Step [8200/12500], Loss: 0.4508\n",
      "Epoch [6/10], Step [8300/12500], Loss: 0.4213\n",
      "Epoch [6/10], Step [8400/12500], Loss: 0.6008\n",
      "Epoch [6/10], Step [8500/12500], Loss: 0.5877\n",
      "Epoch [6/10], Step [8600/12500], Loss: 0.3914\n",
      "Epoch [6/10], Step [8700/12500], Loss: 0.4491\n",
      "Epoch [6/10], Step [8800/12500], Loss: 0.3651\n",
      "Epoch [6/10], Step [8900/12500], Loss: 0.3801\n",
      "Epoch [6/10], Step [9000/12500], Loss: 0.4590\n",
      "Epoch [6/10], Step [9100/12500], Loss: 0.4384\n",
      "Epoch [6/10], Step [9200/12500], Loss: 0.3821\n",
      "Epoch [6/10], Step [9300/12500], Loss: 0.4113\n",
      "Epoch [6/10], Step [9400/12500], Loss: 0.4693\n",
      "Epoch [6/10], Step [9500/12500], Loss: 0.4038\n",
      "Epoch [6/10], Step [9600/12500], Loss: 0.4065\n",
      "Epoch [6/10], Step [9700/12500], Loss: 0.3977\n",
      "Epoch [6/10], Step [9800/12500], Loss: 0.4502\n",
      "Epoch [6/10], Step [9900/12500], Loss: 0.3864\n",
      "Epoch [6/10], Step [10000/12500], Loss: 0.4371\n",
      "Epoch [6/10], Step [10100/12500], Loss: 0.4897\n",
      "Epoch [6/10], Step [10200/12500], Loss: 0.5636\n",
      "Epoch [6/10], Step [10300/12500], Loss: 0.4621\n",
      "Epoch [6/10], Step [10400/12500], Loss: 0.3959\n",
      "Epoch [6/10], Step [10500/12500], Loss: 0.3871\n",
      "Epoch [6/10], Step [10600/12500], Loss: 0.4864\n",
      "Epoch [6/10], Step [10700/12500], Loss: 0.4430\n",
      "Epoch [6/10], Step [10800/12500], Loss: 0.4309\n",
      "Epoch [6/10], Step [10900/12500], Loss: 0.3898\n",
      "Epoch [6/10], Step [11000/12500], Loss: 0.4442\n",
      "Epoch [6/10], Step [11100/12500], Loss: 0.3207\n",
      "Epoch [6/10], Step [11200/12500], Loss: 0.4089\n",
      "Epoch [6/10], Step [11300/12500], Loss: 0.4682\n",
      "Epoch [6/10], Step [11400/12500], Loss: 0.4945\n",
      "Epoch [6/10], Step [11500/12500], Loss: 0.4054\n",
      "Epoch [6/10], Step [11600/12500], Loss: 0.4260\n",
      "Epoch [6/10], Step [11700/12500], Loss: 0.3630\n",
      "Epoch [6/10], Step [11800/12500], Loss: 0.3341\n",
      "Epoch [6/10], Step [11900/12500], Loss: 0.4190\n",
      "Epoch [6/10], Step [12000/12500], Loss: 0.5002\n",
      "Epoch [6/10], Step [12100/12500], Loss: 0.4625\n",
      "Epoch [6/10], Step [12200/12500], Loss: 0.5265\n",
      "Epoch [6/10], Step [12300/12500], Loss: 0.4683\n",
      "Epoch [6/10], Step [12400/12500], Loss: 0.4261\n",
      "Epoch [6/10], Step [12500/12500], Loss: 0.4669\n",
      "Epoch [7/10], Step [100/12500], Loss: 0.2533\n",
      "Epoch [7/10], Step [200/12500], Loss: 0.2619\n",
      "Epoch [7/10], Step [300/12500], Loss: 0.2192\n",
      "Epoch [7/10], Step [400/12500], Loss: 0.2081\n",
      "Epoch [7/10], Step [500/12500], Loss: 0.2228\n",
      "Epoch [7/10], Step [600/12500], Loss: 0.2203\n",
      "Epoch [7/10], Step [700/12500], Loss: 0.2037\n",
      "Epoch [7/10], Step [800/12500], Loss: 0.3307\n",
      "Epoch [7/10], Step [900/12500], Loss: 0.3419\n",
      "Epoch [7/10], Step [1000/12500], Loss: 0.2544\n",
      "Epoch [7/10], Step [1100/12500], Loss: 0.2538\n",
      "Epoch [7/10], Step [1200/12500], Loss: 0.2340\n",
      "Epoch [7/10], Step [1300/12500], Loss: 0.3109\n",
      "Epoch [7/10], Step [1400/12500], Loss: 0.2506\n",
      "Epoch [7/10], Step [1500/12500], Loss: 0.1916\n",
      "Epoch [7/10], Step [1600/12500], Loss: 0.2755\n",
      "Epoch [7/10], Step [1700/12500], Loss: 0.2279\n",
      "Epoch [7/10], Step [1800/12500], Loss: 0.1840\n",
      "Epoch [7/10], Step [1900/12500], Loss: 0.3010\n",
      "Epoch [7/10], Step [2000/12500], Loss: 0.3109\n",
      "Epoch [7/10], Step [2100/12500], Loss: 0.2700\n",
      "Epoch [7/10], Step [2200/12500], Loss: 0.2610\n",
      "Epoch [7/10], Step [2300/12500], Loss: 0.3348\n",
      "Epoch [7/10], Step [2400/12500], Loss: 0.3641\n",
      "Epoch [7/10], Step [2500/12500], Loss: 0.2857\n",
      "Epoch [7/10], Step [2600/12500], Loss: 0.2822\n",
      "Epoch [7/10], Step [2700/12500], Loss: 0.2740\n",
      "Epoch [7/10], Step [2800/12500], Loss: 0.2294\n",
      "Epoch [7/10], Step [2900/12500], Loss: 0.2485\n",
      "Epoch [7/10], Step [3000/12500], Loss: 0.2921\n",
      "Epoch [7/10], Step [3100/12500], Loss: 0.2969\n",
      "Epoch [7/10], Step [3200/12500], Loss: 0.2928\n",
      "Epoch [7/10], Step [3300/12500], Loss: 0.2365\n",
      "Epoch [7/10], Step [3400/12500], Loss: 0.2993\n",
      "Epoch [7/10], Step [3500/12500], Loss: 0.2190\n",
      "Epoch [7/10], Step [3600/12500], Loss: 0.2896\n",
      "Epoch [7/10], Step [3700/12500], Loss: 0.3471\n",
      "Epoch [7/10], Step [3800/12500], Loss: 0.3249\n",
      "Epoch [7/10], Step [3900/12500], Loss: 0.2982\n",
      "Epoch [7/10], Step [4000/12500], Loss: 0.2616\n",
      "Epoch [7/10], Step [4100/12500], Loss: 0.3650\n",
      "Epoch [7/10], Step [4200/12500], Loss: 0.2851\n",
      "Epoch [7/10], Step [4300/12500], Loss: 0.2910\n",
      "Epoch [7/10], Step [4400/12500], Loss: 0.3312\n",
      "Epoch [7/10], Step [4500/12500], Loss: 0.2844\n",
      "Epoch [7/10], Step [4600/12500], Loss: 0.3184\n",
      "Epoch [7/10], Step [4700/12500], Loss: 0.3370\n",
      "Epoch [7/10], Step [4800/12500], Loss: 0.3037\n",
      "Epoch [7/10], Step [4900/12500], Loss: 0.2757\n",
      "Epoch [7/10], Step [5000/12500], Loss: 0.2284\n",
      "Epoch [7/10], Step [5100/12500], Loss: 0.3095\n",
      "Epoch [7/10], Step [5200/12500], Loss: 0.2994\n",
      "Epoch [7/10], Step [5300/12500], Loss: 0.3782\n",
      "Epoch [7/10], Step [5400/12500], Loss: 0.3894\n",
      "Epoch [7/10], Step [5500/12500], Loss: 0.3716\n",
      "Epoch [7/10], Step [5600/12500], Loss: 0.3018\n",
      "Epoch [7/10], Step [5700/12500], Loss: 0.3659\n",
      "Epoch [7/10], Step [5800/12500], Loss: 0.3290\n",
      "Epoch [7/10], Step [5900/12500], Loss: 0.3551\n",
      "Epoch [7/10], Step [6000/12500], Loss: 0.3461\n",
      "Epoch [7/10], Step [6100/12500], Loss: 0.2959\n",
      "Epoch [7/10], Step [6200/12500], Loss: 0.3807\n",
      "Epoch [7/10], Step [6300/12500], Loss: 0.2460\n",
      "Epoch [7/10], Step [6400/12500], Loss: 0.3475\n",
      "Epoch [7/10], Step [6500/12500], Loss: 0.4497\n",
      "Epoch [7/10], Step [6600/12500], Loss: 0.3621\n",
      "Epoch [7/10], Step [6700/12500], Loss: 0.2817\n",
      "Epoch [7/10], Step [6800/12500], Loss: 0.2851\n",
      "Epoch [7/10], Step [6900/12500], Loss: 0.3564\n",
      "Epoch [7/10], Step [7000/12500], Loss: 0.2966\n",
      "Epoch [7/10], Step [7100/12500], Loss: 0.3202\n",
      "Epoch [7/10], Step [7200/12500], Loss: 0.3111\n",
      "Epoch [7/10], Step [7300/12500], Loss: 0.3636\n",
      "Epoch [7/10], Step [7400/12500], Loss: 0.3148\n",
      "Epoch [7/10], Step [7500/12500], Loss: 0.2774\n",
      "Epoch [7/10], Step [7600/12500], Loss: 0.2794\n",
      "Epoch [7/10], Step [7700/12500], Loss: 0.3602\n",
      "Epoch [7/10], Step [7800/12500], Loss: 0.4659\n",
      "Epoch [7/10], Step [7900/12500], Loss: 0.3497\n",
      "Epoch [7/10], Step [8000/12500], Loss: 0.3753\n",
      "Epoch [7/10], Step [8100/12500], Loss: 0.3632\n",
      "Epoch [7/10], Step [8200/12500], Loss: 0.4037\n",
      "Epoch [7/10], Step [8300/12500], Loss: 0.3087\n",
      "Epoch [7/10], Step [8400/12500], Loss: 0.3544\n",
      "Epoch [7/10], Step [8500/12500], Loss: 0.3441\n",
      "Epoch [7/10], Step [8600/12500], Loss: 0.3923\n",
      "Epoch [7/10], Step [8700/12500], Loss: 0.3135\n",
      "Epoch [7/10], Step [8800/12500], Loss: 0.2785\n",
      "Epoch [7/10], Step [8900/12500], Loss: 0.3540\n",
      "Epoch [7/10], Step [9000/12500], Loss: 0.2930\n",
      "Epoch [7/10], Step [9100/12500], Loss: 0.3326\n",
      "Epoch [7/10], Step [9200/12500], Loss: 0.4148\n",
      "Epoch [7/10], Step [9300/12500], Loss: 0.2800\n",
      "Epoch [7/10], Step [9400/12500], Loss: 0.3632\n",
      "Epoch [7/10], Step [9500/12500], Loss: 0.3877\n",
      "Epoch [7/10], Step [9600/12500], Loss: 0.4185\n",
      "Epoch [7/10], Step [9700/12500], Loss: 0.3939\n",
      "Epoch [7/10], Step [9800/12500], Loss: 0.3464\n",
      "Epoch [7/10], Step [9900/12500], Loss: 0.4069\n",
      "Epoch [7/10], Step [10000/12500], Loss: 0.3775\n",
      "Epoch [7/10], Step [10100/12500], Loss: 0.3559\n",
      "Epoch [7/10], Step [10200/12500], Loss: 0.2892\n",
      "Epoch [7/10], Step [10300/12500], Loss: 0.3258\n",
      "Epoch [7/10], Step [10400/12500], Loss: 0.4371\n",
      "Epoch [7/10], Step [10500/12500], Loss: 0.4020\n",
      "Epoch [7/10], Step [10600/12500], Loss: 0.3535\n",
      "Epoch [7/10], Step [10700/12500], Loss: 0.4790\n",
      "Epoch [7/10], Step [10800/12500], Loss: 0.3622\n",
      "Epoch [7/10], Step [10900/12500], Loss: 0.3723\n",
      "Epoch [7/10], Step [11000/12500], Loss: 0.3072\n",
      "Epoch [7/10], Step [11100/12500], Loss: 0.2815\n",
      "Epoch [7/10], Step [11200/12500], Loss: 0.4831\n",
      "Epoch [7/10], Step [11300/12500], Loss: 0.4191\n",
      "Epoch [7/10], Step [11400/12500], Loss: 0.4015\n",
      "Epoch [7/10], Step [11500/12500], Loss: 0.3775\n",
      "Epoch [7/10], Step [11600/12500], Loss: 0.3879\n",
      "Epoch [7/10], Step [11700/12500], Loss: 0.3240\n",
      "Epoch [7/10], Step [11800/12500], Loss: 0.3434\n",
      "Epoch [7/10], Step [11900/12500], Loss: 0.3259\n",
      "Epoch [7/10], Step [12000/12500], Loss: 0.3417\n",
      "Epoch [7/10], Step [12100/12500], Loss: 0.2920\n",
      "Epoch [7/10], Step [12200/12500], Loss: 0.3006\n",
      "Epoch [7/10], Step [12300/12500], Loss: 0.3470\n",
      "Epoch [7/10], Step [12400/12500], Loss: 0.3428\n",
      "Epoch [7/10], Step [12500/12500], Loss: 0.3511\n",
      "Epoch [8/10], Step [100/12500], Loss: 0.1699\n",
      "Epoch [8/10], Step [200/12500], Loss: 0.1870\n",
      "Epoch [8/10], Step [300/12500], Loss: 0.2543\n",
      "Epoch [8/10], Step [400/12500], Loss: 0.2066\n",
      "Epoch [8/10], Step [500/12500], Loss: 0.1938\n",
      "Epoch [8/10], Step [600/12500], Loss: 0.1725\n",
      "Epoch [8/10], Step [700/12500], Loss: 0.2255\n",
      "Epoch [8/10], Step [800/12500], Loss: 0.1884\n",
      "Epoch [8/10], Step [900/12500], Loss: 0.1301\n",
      "Epoch [8/10], Step [1000/12500], Loss: 0.2460\n",
      "Epoch [8/10], Step [1100/12500], Loss: 0.1913\n",
      "Epoch [8/10], Step [1200/12500], Loss: 0.2013\n",
      "Epoch [8/10], Step [1300/12500], Loss: 0.1970\n",
      "Epoch [8/10], Step [1400/12500], Loss: 0.2422\n",
      "Epoch [8/10], Step [1500/12500], Loss: 0.2577\n",
      "Epoch [8/10], Step [1600/12500], Loss: 0.2046\n",
      "Epoch [8/10], Step [1700/12500], Loss: 0.1983\n",
      "Epoch [8/10], Step [1800/12500], Loss: 0.2381\n",
      "Epoch [8/10], Step [1900/12500], Loss: 0.2255\n",
      "Epoch [8/10], Step [2000/12500], Loss: 0.1675\n",
      "Epoch [8/10], Step [2100/12500], Loss: 0.1888\n",
      "Epoch [8/10], Step [2200/12500], Loss: 0.2868\n",
      "Epoch [8/10], Step [2300/12500], Loss: 0.2213\n",
      "Epoch [8/10], Step [2400/12500], Loss: 0.2165\n",
      "Epoch [8/10], Step [2500/12500], Loss: 0.2329\n",
      "Epoch [8/10], Step [2600/12500], Loss: 0.2188\n",
      "Epoch [8/10], Step [2700/12500], Loss: 0.1884\n",
      "Epoch [8/10], Step [2800/12500], Loss: 0.1783\n",
      "Epoch [8/10], Step [2900/12500], Loss: 0.2642\n",
      "Epoch [8/10], Step [3000/12500], Loss: 0.2398\n",
      "Epoch [8/10], Step [3100/12500], Loss: 0.2192\n",
      "Epoch [8/10], Step [3200/12500], Loss: 0.2378\n",
      "Epoch [8/10], Step [3300/12500], Loss: 0.2055\n",
      "Epoch [8/10], Step [3400/12500], Loss: 0.3061\n",
      "Epoch [8/10], Step [3500/12500], Loss: 0.1954\n",
      "Epoch [8/10], Step [3600/12500], Loss: 0.2324\n",
      "Epoch [8/10], Step [3700/12500], Loss: 0.2644\n",
      "Epoch [8/10], Step [3800/12500], Loss: 0.2422\n",
      "Epoch [8/10], Step [3900/12500], Loss: 0.2642\n",
      "Epoch [8/10], Step [4000/12500], Loss: 0.2527\n",
      "Epoch [8/10], Step [4100/12500], Loss: 0.2355\n",
      "Epoch [8/10], Step [4200/12500], Loss: 0.2758\n",
      "Epoch [8/10], Step [4300/12500], Loss: 0.2138\n",
      "Epoch [8/10], Step [4400/12500], Loss: 0.1538\n",
      "Epoch [8/10], Step [4500/12500], Loss: 0.3344\n",
      "Epoch [8/10], Step [4600/12500], Loss: 0.2130\n",
      "Epoch [8/10], Step [4700/12500], Loss: 0.2515\n",
      "Epoch [8/10], Step [4800/12500], Loss: 0.2349\n",
      "Epoch [8/10], Step [4900/12500], Loss: 0.2769\n",
      "Epoch [8/10], Step [5000/12500], Loss: 0.2581\n",
      "Epoch [8/10], Step [5100/12500], Loss: 0.3622\n",
      "Epoch [8/10], Step [5200/12500], Loss: 0.3238\n",
      "Epoch [8/10], Step [5300/12500], Loss: 0.2245\n",
      "Epoch [8/10], Step [5400/12500], Loss: 0.2215\n",
      "Epoch [8/10], Step [5500/12500], Loss: 0.2802\n",
      "Epoch [8/10], Step [5600/12500], Loss: 0.3263\n",
      "Epoch [8/10], Step [5700/12500], Loss: 0.2774\n",
      "Epoch [8/10], Step [5800/12500], Loss: 0.3041\n",
      "Epoch [8/10], Step [5900/12500], Loss: 0.2388\n",
      "Epoch [8/10], Step [6000/12500], Loss: 0.2218\n",
      "Epoch [8/10], Step [6100/12500], Loss: 0.2225\n",
      "Epoch [8/10], Step [6200/12500], Loss: 0.3525\n",
      "Epoch [8/10], Step [6300/12500], Loss: 0.2735\n",
      "Epoch [8/10], Step [6400/12500], Loss: 0.3589\n",
      "Epoch [8/10], Step [6500/12500], Loss: 0.2795\n",
      "Epoch [8/10], Step [6600/12500], Loss: 0.3813\n",
      "Epoch [8/10], Step [6700/12500], Loss: 0.2854\n",
      "Epoch [8/10], Step [6800/12500], Loss: 0.2994\n",
      "Epoch [8/10], Step [6900/12500], Loss: 0.3100\n",
      "Epoch [8/10], Step [7000/12500], Loss: 0.2582\n",
      "Epoch [8/10], Step [7100/12500], Loss: 0.3628\n",
      "Epoch [8/10], Step [7200/12500], Loss: 0.3477\n",
      "Epoch [8/10], Step [7300/12500], Loss: 0.2488\n",
      "Epoch [8/10], Step [7400/12500], Loss: 0.2365\n",
      "Epoch [8/10], Step [7500/12500], Loss: 0.2885\n",
      "Epoch [8/10], Step [7600/12500], Loss: 0.2587\n",
      "Epoch [8/10], Step [7700/12500], Loss: 0.2722\n",
      "Epoch [8/10], Step [7800/12500], Loss: 0.1732\n",
      "Epoch [8/10], Step [7900/12500], Loss: 0.2291\n",
      "Epoch [8/10], Step [8000/12500], Loss: 0.3606\n",
      "Epoch [8/10], Step [8100/12500], Loss: 0.3244\n",
      "Epoch [8/10], Step [8200/12500], Loss: 0.2672\n",
      "Epoch [8/10], Step [8300/12500], Loss: 0.4105\n",
      "Epoch [8/10], Step [8400/12500], Loss: 0.3061\n",
      "Epoch [8/10], Step [8500/12500], Loss: 0.3271\n",
      "Epoch [8/10], Step [8600/12500], Loss: 0.3023\n",
      "Epoch [8/10], Step [8700/12500], Loss: 0.2708\n",
      "Epoch [8/10], Step [8800/12500], Loss: 0.2826\n",
      "Epoch [8/10], Step [8900/12500], Loss: 0.3921\n",
      "Epoch [8/10], Step [9000/12500], Loss: 0.3475\n",
      "Epoch [8/10], Step [9100/12500], Loss: 0.2459\n",
      "Epoch [8/10], Step [9200/12500], Loss: 0.3448\n",
      "Epoch [8/10], Step [9300/12500], Loss: 0.1551\n",
      "Epoch [8/10], Step [9400/12500], Loss: 0.2880\n",
      "Epoch [8/10], Step [9500/12500], Loss: 0.3462\n",
      "Epoch [8/10], Step [9600/12500], Loss: 0.2801\n",
      "Epoch [8/10], Step [9700/12500], Loss: 0.3126\n",
      "Epoch [8/10], Step [9800/12500], Loss: 0.3406\n",
      "Epoch [8/10], Step [9900/12500], Loss: 0.3923\n",
      "Epoch [8/10], Step [10000/12500], Loss: 0.2890\n",
      "Epoch [8/10], Step [10100/12500], Loss: 0.3934\n",
      "Epoch [8/10], Step [10200/12500], Loss: 0.3337\n",
      "Epoch [8/10], Step [10300/12500], Loss: 0.3584\n",
      "Epoch [8/10], Step [10400/12500], Loss: 0.2811\n",
      "Epoch [8/10], Step [10500/12500], Loss: 0.2960\n",
      "Epoch [8/10], Step [10600/12500], Loss: 0.2114\n",
      "Epoch [8/10], Step [10700/12500], Loss: 0.2642\n",
      "Epoch [8/10], Step [10800/12500], Loss: 0.2831\n",
      "Epoch [8/10], Step [10900/12500], Loss: 0.4055\n",
      "Epoch [8/10], Step [11000/12500], Loss: 0.3201\n",
      "Epoch [8/10], Step [11100/12500], Loss: 0.4576\n",
      "Epoch [8/10], Step [11200/12500], Loss: 0.3992\n",
      "Epoch [8/10], Step [11300/12500], Loss: 0.3424\n",
      "Epoch [8/10], Step [11400/12500], Loss: 0.3328\n",
      "Epoch [8/10], Step [11500/12500], Loss: 0.3515\n",
      "Epoch [8/10], Step [11600/12500], Loss: 0.3593\n",
      "Epoch [8/10], Step [11700/12500], Loss: 0.2626\n",
      "Epoch [8/10], Step [11800/12500], Loss: 0.3779\n",
      "Epoch [8/10], Step [11900/12500], Loss: 0.2836\n",
      "Epoch [8/10], Step [12000/12500], Loss: 0.2477\n",
      "Epoch [8/10], Step [12100/12500], Loss: 0.2934\n",
      "Epoch [8/10], Step [12200/12500], Loss: 0.2913\n",
      "Epoch [8/10], Step [12300/12500], Loss: 0.3010\n",
      "Epoch [8/10], Step [12400/12500], Loss: 0.4872\n",
      "Epoch [8/10], Step [12500/12500], Loss: 0.3282\n",
      "Epoch [9/10], Step [100/12500], Loss: 0.1688\n",
      "Epoch [9/10], Step [200/12500], Loss: 0.1128\n",
      "Epoch [9/10], Step [300/12500], Loss: 0.1591\n",
      "Epoch [9/10], Step [400/12500], Loss: 0.2258\n",
      "Epoch [9/10], Step [500/12500], Loss: 0.1632\n",
      "Epoch [9/10], Step [600/12500], Loss: 0.1655\n",
      "Epoch [9/10], Step [700/12500], Loss: 0.2743\n",
      "Epoch [9/10], Step [800/12500], Loss: 0.1774\n",
      "Epoch [9/10], Step [900/12500], Loss: 0.1815\n",
      "Epoch [9/10], Step [1000/12500], Loss: 0.2352\n",
      "Epoch [9/10], Step [1100/12500], Loss: 0.1410\n",
      "Epoch [9/10], Step [1200/12500], Loss: 0.2160\n",
      "Epoch [9/10], Step [1300/12500], Loss: 0.2227\n",
      "Epoch [9/10], Step [1400/12500], Loss: 0.1560\n",
      "Epoch [9/10], Step [1500/12500], Loss: 0.1382\n",
      "Epoch [9/10], Step [1600/12500], Loss: 0.1465\n",
      "Epoch [9/10], Step [1700/12500], Loss: 0.2114\n",
      "Epoch [9/10], Step [1800/12500], Loss: 0.1853\n",
      "Epoch [9/10], Step [1900/12500], Loss: 0.1871\n",
      "Epoch [9/10], Step [2000/12500], Loss: 0.2054\n",
      "Epoch [9/10], Step [2100/12500], Loss: 0.2604\n",
      "Epoch [9/10], Step [2200/12500], Loss: 0.2554\n",
      "Epoch [9/10], Step [2300/12500], Loss: 0.1656\n",
      "Epoch [9/10], Step [2400/12500], Loss: 0.1479\n",
      "Epoch [9/10], Step [2500/12500], Loss: 0.1993\n",
      "Epoch [9/10], Step [2600/12500], Loss: 0.2409\n",
      "Epoch [9/10], Step [2700/12500], Loss: 0.1732\n",
      "Epoch [9/10], Step [2800/12500], Loss: 0.2030\n",
      "Epoch [9/10], Step [2900/12500], Loss: 0.1758\n",
      "Epoch [9/10], Step [3000/12500], Loss: 0.2549\n",
      "Epoch [9/10], Step [3100/12500], Loss: 0.2504\n",
      "Epoch [9/10], Step [3200/12500], Loss: 0.2349\n",
      "Epoch [9/10], Step [3300/12500], Loss: 0.2661\n",
      "Epoch [9/10], Step [3400/12500], Loss: 0.2230\n",
      "Epoch [9/10], Step [3500/12500], Loss: 0.3195\n",
      "Epoch [9/10], Step [3600/12500], Loss: 0.2308\n",
      "Epoch [9/10], Step [3700/12500], Loss: 0.1953\n",
      "Epoch [9/10], Step [3800/12500], Loss: 0.2296\n",
      "Epoch [9/10], Step [3900/12500], Loss: 0.2386\n",
      "Epoch [9/10], Step [4000/12500], Loss: 0.1951\n",
      "Epoch [9/10], Step [4100/12500], Loss: 0.2438\n",
      "Epoch [9/10], Step [4200/12500], Loss: 0.2288\n",
      "Epoch [9/10], Step [4300/12500], Loss: 0.2102\n",
      "Epoch [9/10], Step [4400/12500], Loss: 0.1909\n",
      "Epoch [9/10], Step [4500/12500], Loss: 0.2523\n",
      "Epoch [9/10], Step [4600/12500], Loss: 0.1966\n",
      "Epoch [9/10], Step [4700/12500], Loss: 0.1764\n",
      "Epoch [9/10], Step [4800/12500], Loss: 0.2130\n",
      "Epoch [9/10], Step [4900/12500], Loss: 0.1825\n",
      "Epoch [9/10], Step [5000/12500], Loss: 0.2461\n",
      "Epoch [9/10], Step [5100/12500], Loss: 0.2744\n",
      "Epoch [9/10], Step [5200/12500], Loss: 0.2098\n",
      "Epoch [9/10], Step [5300/12500], Loss: 0.1907\n",
      "Epoch [9/10], Step [5400/12500], Loss: 0.2830\n",
      "Epoch [9/10], Step [5500/12500], Loss: 0.1569\n",
      "Epoch [9/10], Step [5600/12500], Loss: 0.1755\n",
      "Epoch [9/10], Step [5700/12500], Loss: 0.1835\n",
      "Epoch [9/10], Step [5800/12500], Loss: 0.2468\n",
      "Epoch [9/10], Step [5900/12500], Loss: 0.2643\n",
      "Epoch [9/10], Step [6000/12500], Loss: 0.2161\n",
      "Epoch [9/10], Step [6100/12500], Loss: 0.2941\n",
      "Epoch [9/10], Step [6200/12500], Loss: 0.2594\n",
      "Epoch [9/10], Step [6300/12500], Loss: 0.1961\n",
      "Epoch [9/10], Step [6400/12500], Loss: 0.2428\n",
      "Epoch [9/10], Step [6500/12500], Loss: 0.2571\n",
      "Epoch [9/10], Step [6600/12500], Loss: 0.2621\n",
      "Epoch [9/10], Step [6700/12500], Loss: 0.2253\n",
      "Epoch [9/10], Step [6800/12500], Loss: 0.2020\n",
      "Epoch [9/10], Step [6900/12500], Loss: 0.2075\n",
      "Epoch [9/10], Step [7000/12500], Loss: 0.2726\n",
      "Epoch [9/10], Step [7100/12500], Loss: 0.2563\n",
      "Epoch [9/10], Step [7200/12500], Loss: 0.2750\n",
      "Epoch [9/10], Step [7300/12500], Loss: 0.1958\n",
      "Epoch [9/10], Step [7400/12500], Loss: 0.2292\n",
      "Epoch [9/10], Step [7500/12500], Loss: 0.2983\n",
      "Epoch [9/10], Step [7600/12500], Loss: 0.1907\n",
      "Epoch [9/10], Step [7700/12500], Loss: 0.1914\n",
      "Epoch [9/10], Step [7800/12500], Loss: 0.3716\n",
      "Epoch [9/10], Step [7900/12500], Loss: 0.3445\n",
      "Epoch [9/10], Step [8000/12500], Loss: 0.2941\n",
      "Epoch [9/10], Step [8100/12500], Loss: 0.2390\n",
      "Epoch [9/10], Step [8200/12500], Loss: 0.2448\n",
      "Epoch [9/10], Step [8300/12500], Loss: 0.2161\n",
      "Epoch [9/10], Step [8400/12500], Loss: 0.2259\n",
      "Epoch [9/10], Step [8500/12500], Loss: 0.2448\n",
      "Epoch [9/10], Step [8600/12500], Loss: 0.2797\n",
      "Epoch [9/10], Step [8700/12500], Loss: 0.1989\n",
      "Epoch [9/10], Step [8800/12500], Loss: 0.2823\n",
      "Epoch [9/10], Step [8900/12500], Loss: 0.2841\n",
      "Epoch [9/10], Step [9000/12500], Loss: 0.3244\n",
      "Epoch [9/10], Step [9100/12500], Loss: 0.2665\n",
      "Epoch [9/10], Step [9200/12500], Loss: 0.3193\n",
      "Epoch [9/10], Step [9300/12500], Loss: 0.1469\n",
      "Epoch [9/10], Step [9400/12500], Loss: 0.3777\n",
      "Epoch [9/10], Step [9500/12500], Loss: 0.3016\n",
      "Epoch [9/10], Step [9600/12500], Loss: 0.2060\n",
      "Epoch [9/10], Step [9700/12500], Loss: 0.2403\n",
      "Epoch [9/10], Step [9800/12500], Loss: 0.2714\n",
      "Epoch [9/10], Step [9900/12500], Loss: 0.2895\n",
      "Epoch [9/10], Step [10000/12500], Loss: 0.2698\n",
      "Epoch [9/10], Step [10100/12500], Loss: 0.2598\n",
      "Epoch [9/10], Step [10200/12500], Loss: 0.3513\n",
      "Epoch [9/10], Step [10300/12500], Loss: 0.2693\n",
      "Epoch [9/10], Step [10400/12500], Loss: 0.3897\n",
      "Epoch [9/10], Step [10500/12500], Loss: 0.3415\n",
      "Epoch [9/10], Step [10600/12500], Loss: 0.2686\n",
      "Epoch [9/10], Step [10700/12500], Loss: 0.2279\n",
      "Epoch [9/10], Step [10800/12500], Loss: 0.2720\n",
      "Epoch [9/10], Step [10900/12500], Loss: 0.3067\n",
      "Epoch [9/10], Step [11000/12500], Loss: 0.2835\n",
      "Epoch [9/10], Step [11100/12500], Loss: 0.2002\n",
      "Epoch [9/10], Step [11200/12500], Loss: 0.2569\n",
      "Epoch [9/10], Step [11300/12500], Loss: 0.2691\n",
      "Epoch [9/10], Step [11400/12500], Loss: 0.2740\n",
      "Epoch [9/10], Step [11500/12500], Loss: 0.2164\n",
      "Epoch [9/10], Step [11600/12500], Loss: 0.3742\n",
      "Epoch [9/10], Step [11700/12500], Loss: 0.2543\n",
      "Epoch [9/10], Step [11800/12500], Loss: 0.3242\n",
      "Epoch [9/10], Step [11900/12500], Loss: 0.2983\n",
      "Epoch [9/10], Step [12000/12500], Loss: 0.2745\n",
      "Epoch [9/10], Step [12100/12500], Loss: 0.2639\n",
      "Epoch [9/10], Step [12200/12500], Loss: 0.2567\n",
      "Epoch [9/10], Step [12300/12500], Loss: 0.2998\n",
      "Epoch [9/10], Step [12400/12500], Loss: 0.2753\n",
      "Epoch [9/10], Step [12500/12500], Loss: 0.2051\n",
      "Epoch [10/10], Step [100/12500], Loss: 0.1343\n",
      "Epoch [10/10], Step [200/12500], Loss: 0.1238\n",
      "Epoch [10/10], Step [300/12500], Loss: 0.1028\n",
      "Epoch [10/10], Step [400/12500], Loss: 0.1466\n",
      "Epoch [10/10], Step [500/12500], Loss: 0.1368\n",
      "Epoch [10/10], Step [600/12500], Loss: 0.1282\n",
      "Epoch [10/10], Step [700/12500], Loss: 0.1890\n",
      "Epoch [10/10], Step [800/12500], Loss: 0.1238\n",
      "Epoch [10/10], Step [900/12500], Loss: 0.1719\n",
      "Epoch [10/10], Step [1000/12500], Loss: 0.1772\n",
      "Epoch [10/10], Step [1100/12500], Loss: 0.2048\n",
      "Epoch [10/10], Step [1200/12500], Loss: 0.2125\n",
      "Epoch [10/10], Step [1300/12500], Loss: 0.1217\n",
      "Epoch [10/10], Step [1400/12500], Loss: 0.1217\n",
      "Epoch [10/10], Step [1500/12500], Loss: 0.1536\n",
      "Epoch [10/10], Step [1600/12500], Loss: 0.2098\n",
      "Epoch [10/10], Step [1700/12500], Loss: 0.2315\n",
      "Epoch [10/10], Step [1800/12500], Loss: 0.2142\n",
      "Epoch [10/10], Step [1900/12500], Loss: 0.2413\n",
      "Epoch [10/10], Step [2000/12500], Loss: 0.1431\n",
      "Epoch [10/10], Step [2100/12500], Loss: 0.1200\n",
      "Epoch [10/10], Step [2200/12500], Loss: 0.1247\n",
      "Epoch [10/10], Step [2300/12500], Loss: 0.1789\n",
      "Epoch [10/10], Step [2400/12500], Loss: 0.1594\n",
      "Epoch [10/10], Step [2500/12500], Loss: 0.1811\n",
      "Epoch [10/10], Step [2600/12500], Loss: 0.1640\n",
      "Epoch [10/10], Step [2700/12500], Loss: 0.1179\n",
      "Epoch [10/10], Step [2800/12500], Loss: 0.1178\n",
      "Epoch [10/10], Step [2900/12500], Loss: 0.2878\n",
      "Epoch [10/10], Step [3000/12500], Loss: 0.2551\n",
      "Epoch [10/10], Step [3100/12500], Loss: 0.1897\n",
      "Epoch [10/10], Step [3200/12500], Loss: 0.2148\n",
      "Epoch [10/10], Step [3300/12500], Loss: 0.2952\n",
      "Epoch [10/10], Step [3400/12500], Loss: 0.2418\n",
      "Epoch [10/10], Step [3500/12500], Loss: 0.1401\n",
      "Epoch [10/10], Step [3600/12500], Loss: 0.2257\n",
      "Epoch [10/10], Step [3700/12500], Loss: 0.1951\n",
      "Epoch [10/10], Step [3800/12500], Loss: 0.1353\n",
      "Epoch [10/10], Step [3900/12500], Loss: 0.1865\n",
      "Epoch [10/10], Step [4000/12500], Loss: 0.1952\n",
      "Epoch [10/10], Step [4100/12500], Loss: 0.1949\n",
      "Epoch [10/10], Step [4200/12500], Loss: 0.2668\n",
      "Epoch [10/10], Step [4300/12500], Loss: 0.2046\n",
      "Epoch [10/10], Step [4400/12500], Loss: 0.2045\n",
      "Epoch [10/10], Step [4500/12500], Loss: 0.1911\n",
      "Epoch [10/10], Step [4600/12500], Loss: 0.2316\n",
      "Epoch [10/10], Step [4700/12500], Loss: 0.2353\n",
      "Epoch [10/10], Step [4800/12500], Loss: 0.2745\n",
      "Epoch [10/10], Step [4900/12500], Loss: 0.1837\n",
      "Epoch [10/10], Step [5000/12500], Loss: 0.1984\n",
      "Epoch [10/10], Step [5100/12500], Loss: 0.2650\n",
      "Epoch [10/10], Step [5200/12500], Loss: 0.2358\n",
      "Epoch [10/10], Step [5300/12500], Loss: 0.2033\n",
      "Epoch [10/10], Step [5400/12500], Loss: 0.1588\n",
      "Epoch [10/10], Step [5500/12500], Loss: 0.2496\n",
      "Epoch [10/10], Step [5600/12500], Loss: 0.1884\n",
      "Epoch [10/10], Step [5700/12500], Loss: 0.2035\n",
      "Epoch [10/10], Step [5800/12500], Loss: 0.2430\n",
      "Epoch [10/10], Step [5900/12500], Loss: 0.1493\n",
      "Epoch [10/10], Step [6000/12500], Loss: 0.1934\n",
      "Epoch [10/10], Step [6100/12500], Loss: 0.2269\n",
      "Epoch [10/10], Step [6200/12500], Loss: 0.2240\n",
      "Epoch [10/10], Step [6300/12500], Loss: 0.2219\n",
      "Epoch [10/10], Step [6400/12500], Loss: 0.2228\n",
      "Epoch [10/10], Step [6500/12500], Loss: 0.1924\n",
      "Epoch [10/10], Step [6600/12500], Loss: 0.2593\n",
      "Epoch [10/10], Step [6700/12500], Loss: 0.2539\n",
      "Epoch [10/10], Step [6800/12500], Loss: 0.2308\n",
      "Epoch [10/10], Step [6900/12500], Loss: 0.3572\n",
      "Epoch [10/10], Step [7000/12500], Loss: 0.3300\n",
      "Epoch [10/10], Step [7100/12500], Loss: 0.1429\n",
      "Epoch [10/10], Step [7200/12500], Loss: 0.1754\n",
      "Epoch [10/10], Step [7300/12500], Loss: 0.1721\n",
      "Epoch [10/10], Step [7400/12500], Loss: 0.3516\n",
      "Epoch [10/10], Step [7500/12500], Loss: 0.2403\n",
      "Epoch [10/10], Step [7600/12500], Loss: 0.2492\n",
      "Epoch [10/10], Step [7700/12500], Loss: 0.2096\n",
      "Epoch [10/10], Step [7800/12500], Loss: 0.2872\n",
      "Epoch [10/10], Step [7900/12500], Loss: 0.1984\n",
      "Epoch [10/10], Step [8000/12500], Loss: 0.2101\n",
      "Epoch [10/10], Step [8100/12500], Loss: 0.1994\n",
      "Epoch [10/10], Step [8200/12500], Loss: 0.2500\n",
      "Epoch [10/10], Step [8300/12500], Loss: 0.1639\n",
      "Epoch [10/10], Step [8400/12500], Loss: 0.1963\n",
      "Epoch [10/10], Step [8500/12500], Loss: 0.2126\n",
      "Epoch [10/10], Step [8600/12500], Loss: 0.3914\n",
      "Epoch [10/10], Step [8700/12500], Loss: 0.2056\n",
      "Epoch [10/10], Step [8800/12500], Loss: 0.2509\n",
      "Epoch [10/10], Step [8900/12500], Loss: 0.1775\n",
      "Epoch [10/10], Step [9000/12500], Loss: 0.2072\n",
      "Epoch [10/10], Step [9100/12500], Loss: 0.1981\n",
      "Epoch [10/10], Step [9200/12500], Loss: 0.1830\n",
      "Epoch [10/10], Step [9300/12500], Loss: 0.1752\n",
      "Epoch [10/10], Step [9400/12500], Loss: 0.2681\n",
      "Epoch [10/10], Step [9500/12500], Loss: 0.1955\n",
      "Epoch [10/10], Step [9600/12500], Loss: 0.1408\n",
      "Epoch [10/10], Step [9700/12500], Loss: 0.1962\n",
      "Epoch [10/10], Step [9800/12500], Loss: 0.1964\n",
      "Epoch [10/10], Step [9900/12500], Loss: 0.1858\n",
      "Epoch [10/10], Step [10000/12500], Loss: 0.3678\n",
      "Epoch [10/10], Step [10100/12500], Loss: 0.2428\n",
      "Epoch [10/10], Step [10200/12500], Loss: 0.2497\n",
      "Epoch [10/10], Step [10300/12500], Loss: 0.3012\n",
      "Epoch [10/10], Step [10400/12500], Loss: 0.2543\n",
      "Epoch [10/10], Step [10500/12500], Loss: 0.2169\n",
      "Epoch [10/10], Step [10600/12500], Loss: 0.1717\n",
      "Epoch [10/10], Step [10700/12500], Loss: 0.2488\n",
      "Epoch [10/10], Step [10800/12500], Loss: 0.1807\n",
      "Epoch [10/10], Step [10900/12500], Loss: 0.3095\n",
      "Epoch [10/10], Step [11000/12500], Loss: 0.2527\n",
      "Epoch [10/10], Step [11100/12500], Loss: 0.3715\n",
      "Epoch [10/10], Step [11200/12500], Loss: 0.2693\n",
      "Epoch [10/10], Step [11300/12500], Loss: 0.2295\n",
      "Epoch [10/10], Step [11400/12500], Loss: 0.2261\n",
      "Epoch [10/10], Step [11500/12500], Loss: 0.2382\n",
      "Epoch [10/10], Step [11600/12500], Loss: 0.2157\n",
      "Epoch [10/10], Step [11700/12500], Loss: 0.2642\n",
      "Epoch [10/10], Step [11800/12500], Loss: 0.2275\n",
      "Epoch [10/10], Step [11900/12500], Loss: 0.2845\n",
      "Epoch [10/10], Step [12000/12500], Loss: 0.2680\n",
      "Epoch [10/10], Step [12100/12500], Loss: 0.2227\n",
      "Epoch [10/10], Step [12200/12500], Loss: 0.2831\n",
      "Epoch [10/10], Step [12300/12500], Loss: 0.2351\n",
      "Epoch [10/10], Step [12400/12500], Loss: 0.2024\n",
      "Epoch [10/10], Step [12500/12500], Loss: 0.2955\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 60\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 100 mini-batches\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(trainloader)}], Loss: {running_loss / 100:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "441e710f-710a-4986-a9eb-341486f600de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.2941, Train Accuracy: 65.07%, Test Accuracy: 62.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.8965, Train Accuracy: 75.38%, Test Accuracy: 69.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.6948, Train Accuracy: 82.22%, Test Accuracy: 72.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.5328, Train Accuracy: 88.48%, Test Accuracy: 74.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.3798, Train Accuracy: 92.95%, Test Accuracy: 73.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.2455, Train Accuracy: 94.78%, Test Accuracy: 72.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.1529, Train Accuracy: 97.20%, Test Accuracy: 72.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.1011, Train Accuracy: 97.91%, Test Accuracy: 72.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.0791, Train Accuracy: 97.90%, Test Accuracy: 71.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.0657, Train Accuracy: 98.58%, Test Accuracy: 72.31%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoaders\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Define the model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Function to compute accuracy\n",
    "def compute_accuracy(loader, model, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "    \n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and validation\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_bar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{num_epochs} Train\", leave=False)\n",
    "    \n",
    "    for inputs, labels in train_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        train_bar.set_postfix(loss=running_loss / (train_bar.n + 1))\n",
    "        \n",
    "    # Compute training accuracy\n",
    "    train_accuracy = compute_accuracy(trainloader, model, device)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    test_accuracy = compute_accuracy(testloader, model, device)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(trainloader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9db516-978a-48a5-b27a-e03a2412e984",
   "metadata": {},
   "source": [
    "#### Summary: What does it mean to train a model?\n",
    "\n",
    "A model is a programmatically-defined function whose outputs depend on two things:\n",
    "\n",
    "- An input (think, an image to process)\n",
    "- Parameters (the coefficients in the huge list of equations that a model uses internally to process data)\n",
    "\n",
    "Training is optimization: we're searching for a special set of parameters that cause the model to mostly give the right answers for most inputs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "004de0f9-c92c-4e59-8441-54a021492939",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://imgs.xkcd.com/comics/machine_learning_2x.png\" width=\"360\"/>\n",
    "</div>\n",
    "\n",
    "Link: [xkcd 1838](https://xkcd.com/1838)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b4f2c-a06d-4ae7-ad7c-39da242d2cd8",
   "metadata": {},
   "source": [
    "#### Summary: Elements of Training in PyTorch\n",
    "\n",
    "We used an LLM to generate most of this code. Any working solution should have all of these pieces:\n",
    "\n",
    "- Dealing with data:\n",
    "  - `torch.utils.data.Dataset` : a list of data and labels\n",
    "  - `torch.utils.data.random_split` : a function that randomly splits your data into train and test sets, if your dataset doesn't come with a split given to you\n",
    "  - `torch.utils.data.DataLoader` : a wrapper around a `Dataset` that generates random \"batches\" as needed\n",
    "- Setting up a model:\n",
    "  - `torch.nn.Module` : code that says how your model processes input and creates output\n",
    "- A \"training loop\" that does model training. (outer loop: epochs. inner loop: batches.)\n",
    "  - `torch.nn.Loss` : a small function that decides how far the model's output is from the right answer\n",
    "  - `torch.nn.Optimizer` : a choice of an algorithm that searches for a set of Parameters that work\n",
    "\n",
    "**Vocab**: an *epoch* is one pass through the training dataset. A model that has been trained for one epoch has seen each image in the training dataset exactly once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f02b237-dfd0-4f2a-ba31-8732fed875dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csse461",
   "language": "python",
   "name": "csse461"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
